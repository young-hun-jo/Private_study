{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tropical-prior",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#스키마\" data-toc-modified-id=\"스키마-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>스키마</a></span></li><li><span><a href=\"#칼럼과-표현식\" data-toc-modified-id=\"칼럼과-표현식-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>칼럼과 표현식</a></span><ul class=\"toc-item\"><li><span><a href=\"#칼럼\" data-toc-modified-id=\"칼럼-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>칼럼</a></span></li><li><span><a href=\"#표현식\" data-toc-modified-id=\"표현식-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>표현식</a></span></li><li><span><a href=\"#레코드와-로우\" data-toc-modified-id=\"레코드와-로우-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>레코드와 로우</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lucky-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = spark.read.format('json').load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inappropriate-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 살펴보기\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-firewall",
   "metadata": {},
   "source": [
    "# 스키마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "academic-chamber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('json').load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json').schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-movement",
   "metadata": {},
   "source": [
    "- 스키마를 출력한 결과\n",
    "    * ``StructType``이라는 리스트 안에 각 칼럼별로 ``StructField``가 담겨져 있음\n",
    "    * 즉, 스키마 = 여러개의 ``StructField`` 타입 필드로 구성된 ``StructType`` 객체\n",
    "- 데이터 타입과 스키마의 데이터 타입이 불일치하면 스파크에서는 오류 발생\n",
    "- 스파크에서는 자체 데이터 타입을 사용하므로 사용하는 언어 API(ex.Python, R 등)의 데이터 타입을 사용할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-rental",
   "metadata": {},
   "source": [
    "- 스키마 정의시 **메타 데이터**를 정의할 수도 있음\n",
    "    * **메타 데이터란, 해당 칼럼과 관련된 정보이며 추후 스파크의 머신러닝 라이브러리에서 사용함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informed-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임에 스키마를 만들고 적용하는 예제\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('count', LongType(), False, metadata={'hello': 'world'})\n",
    "])\n",
    "\n",
    "# 데이터 로드 시 스키마 정의\n",
    "df = spark.read.format('json').schema(myManualSchema)\\\n",
    "     .load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-tract",
   "metadata": {},
   "source": [
    "# 칼럼과 표현식\n",
    "- 표현식으로 데이터프레임의 칼럼을 선택, 조작, 제거 가능\n",
    "- 스파크의 칼럼은 표현식을 사용해 레코드(Row) 단위로 계산한 값을 단순하게 나타내는 논리적인 구조\n",
    "- 칼럼의 실제값을 얻으려면 레코드(Row)가 필요하고 레코드를 얻으려면 데이터프레임이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-lobby",
   "metadata": {},
   "source": [
    "## 칼럼\n",
    "\n",
    "- 칼럼을 생성, 참조할 수 있는 방법은 여러가지가 있지만 ``col()``이나 ``column()``을 사용하는 것이 가장 간단. 소괄호 인자에는 ``칼럼명``을 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proud-slide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'someColumnName'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "col('someColumnName')\n",
    "column('someColumnName')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-portfolio",
   "metadata": {},
   "source": [
    "- 이 때 해당 칼럼이 로드한 DataFrame에 있는지 여부는 알 수 없음. 알려면 **카탈로그**에 저장된 정보와 비교해야 함. 그런데 이 비교하는 단계는 **분석기**가 트랜스포메이션으로 구성된 논리적실행계획을 검증 전/후로 가는 단계에 걸쳐있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-posting",
   "metadata": {},
   "source": [
    "- 명시적 컬럼 참조. ``col()`` 사용. 이는 데이터프레임 조인 시 특정 칼럼을 참조하는 데 사용\n",
    "- 단, ``col()``을 사용해 명시적으로 컬럼을 정의하면 **분석기** 실행 단계에서 컬럼 확인 절차를 생략함!(그러므로 더 빨라지겠지..?)\n",
    "- Pyspark에서는 ``df['column_name']``을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "geographic-deployment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-migration",
   "metadata": {},
   "source": [
    "## 표현식\n",
    "- 표현식이란, 데이터프레임의 레코드의 여러값에 대한 **트랜스포메이션의 집합**\n",
    "- ``expr()``과 ``col()``로 특정 칼럼을 참조하는 것은 동일\n",
    "- 예를 들어, ``expr('SomeCol - 5')`` == ``col('someCol') - 5`` == ``expr(SomeCol') - 5``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dressed-whole",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((SomeCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "expr(\"(((SomeCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-rhythm",
   "metadata": {},
   "source": [
    "- 위 데이터프레임 코드는 SQL의 SELECT 구문으로 해당 표현식을 동일하게 사용해도 동일한 결과를 생성. 왜냐하면 실행 시점에 동일한 논리 트리로 컴파일되기 때문. 성능도 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "moved-royal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임의 컬럼에 접근해보기\n",
    "spark.read.format('json').load('/Users/younghun/Desktop/gitrepo/data/spark_perfect_guide/flight-data/json/2015-summary.json')\\\n",
    "     .columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-format",
   "metadata": {},
   "source": [
    "## 레코드와 로우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "according-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 첫 번째 레코드(Row 객체)를 반환\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-writer",
   "metadata": {},
   "source": [
    "- Row객체인 레코드를 생성해보자\n",
    "- 단, Row는 스키마를 갖고 있지 않고 데이터프레임만 스키마를 가짐. 따라서 Row 생성 시, 데이터프레임의 명시된 스키마 순서와 동일하게 해주어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "environmental-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myRow = Row('Hello', None, 1, False) # 마지막 False는 nullable에 대한 인자!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "becoming-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello None\n"
     ]
    }
   ],
   "source": [
    "# 로우의 데이터에 접근하기\n",
    "print(myRow[0], myRow[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "needed-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "Row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-commodity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
