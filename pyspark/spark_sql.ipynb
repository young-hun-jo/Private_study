{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "## Starting point : SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark에서의 모든 functionality에 들어가는 entry point는 SparkSession이라는 클래스이다.\n",
    "- 그래서 기본 SparkSession을 만들기 위해 ``SparkSession.builder``사용\n",
    "- 만약 가상환경에서 Pyspark를 설치했다면 실행도 가상환경에서 실행해주어야 함!\n",
    "    * **왜냐하면 Pyspark 설치했을 때의 Python 버전과 실행했을 때의 Python 버전이 일치해야 함! 아니면 에러 발생!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Python Spark SQL basic example')\\\n",
    "        .config('spark.some.config.option', 'some-value')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Create json file using spark\n",
    "# sparkContext로 객체 생성\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# json 파일 읽어들이기\n",
    "path = '/Users/younghun/Desktop/gitrepo/TIL/pyspark/people.json'\n",
    "peopleDF = spark.read.json(path)\n",
    "\n",
    "# printSchema()로 json파일의 스키마 형태 볼수 있음\n",
    "peopleDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임을 사용하는 임시의 view(가상의 테이블) 생성\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# spark에서 제공하는 sql 메소드를 이용해 쿼리 날리기\n",
    "# 쿼리문에서 people 테이블은 위에서 만들었던 view 테이블임!\n",
    "teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagerNamesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|[Columbus, Ohio]| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 또한 데이터프레임은 RDD[String] 자료구조를 이용해서 json 데이터셋을 데이터프레임으로 만들 수 있음\n",
    "jsonStrings = ['{\"name\": \"Yin\", \"address\":{\"city\":\"Columbus\", \"state\":\"Ohio\"}}']\n",
    "# json -> RDD형식으로 만들기\n",
    "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "# json파일 읽어오기\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untyped Dataset Operations\n",
    "- a.k.a DataFrame Operations\n",
    "<br><br>\n",
    "- 데이터프레임의 칼럼에 접근하기 위한 방법\n",
    "    * df.column \n",
    "    * df['column'] : 이 방법이 사람들이 더 많이 사용\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임의 스키마 미리보기\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# name 칼럼 select 해서 살펴보기\n",
    "df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# name과 age 칼럼값들을 보는데, age에 1을 더하는 연산 취해 새로운 칼럼 만들어 select\n",
    "# 단, null값에는 더해도 null\n",
    "df.select(df['name'], df['age']+1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "\n",
    "- 특정 조건을 만족하는 행 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age'] > 20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "- 특정 칼럼으로 그룹핑해서 count하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('age').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   name|count|\n",
      "+-------+-----+\n",
      "|Michael|    1|\n",
      "|   Andy|    1|\n",
      "| Justin|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('name').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries\n",
    "\n",
    "- spark로 sql 쿼리를 날리면 데이터프레임 형태로 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view로 가상의 테이블 생성\n",
    "df.createOrReplaceTempView('people') # people = 테이블 이름\n",
    "\n",
    "sqlDF = spark.sql('SELECT * FROM people')\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Temporary View\n",
    "\n",
    "- 일반 Temporary view의 가상테이블은 ``SparkSession``이 종료되면 삭제됨\n",
    "- 하지만 모든 ``SparkSession``들 간에 view 가상 테이블을 공유하고 싶다면 Global Temporary view 사용\n",
    "- Global Temporary view는 시스템 내부에 보존되는 데이터베이스인 ``global_temp``와 연결되어 있어서 테이블 앞에 ``gobal_temp``를 붙여주어야 한다<br>ex) SELECT * FROM global_temp.view1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global Temporary View 생성\n",
    "df.createOrReplaceGlobalTempView('people')\n",
    "\n",
    "sqlDF = spark.sql('SELECT * FROM global_temp.people')\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection을 사용해 Schema 만들기\n",
    "\n",
    "- Row 메소드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines의 type: <class 'pyspark.rdd.RDD'>\n",
      "teenagers type: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "teenNames type: <class 'list'>\n",
      "['Name: Justin']\n",
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# SparkContext 객체 만들기\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# txt file 한 줄씩 읽어오기\n",
    "lines = sc.textFile('./people.txt')\n",
    "print('lines의 type:', type(lines)) # RDD 자료구조\n",
    "\n",
    "# 읽어온 한 줄마다 map함수를 사용해 전처리 - ,구분자로 구분해 리스트로 담기\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "\n",
    "# Row를 통해 name이라는 칼럼명에는 P리스트의 첫 번째 값을 넣기\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# 스키마를 형성하고 데이터프레임으로 생성\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "# View 테이블로 만들기\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "\n",
    "# 만든 View 테이블에서 원하는 데이터 추출 -> SQL결과는 데이터프레임 객체로 반환\n",
    "teenagers = spark.sql('SELECT name FROM people WHERE age >= 13 AND age <= 19')\n",
    "print('teenagers type:', type(teenagers))\n",
    "\n",
    "# RDD는 데이터프레임의 값(content)를 반환함\n",
    "# 여기서 p는 teenagers라는 데이터프레임의 각 row를 의미\n",
    "teenNames = teenagers.rdd.map(lambda p: 'Name: ' + p.name).collect()\n",
    "print('teenNames type:', type(teenNames)) # list로 반환됨!\n",
    "print(teenNames)\n",
    "\n",
    "for name in teenNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema를 programmatical하게 명시하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tuple, List의 RDD를 생성\n",
    "2. 1단계에서 만들어진 RDD안에 존재하는 tuple, list의 구조를 매칭시키는 ``StructType``에 의해 나타내지는 스키마를 생성\n",
    "3. SparkSession이 제공하는 메소드인 ``createDataFrame``을 활용해 2단계에서 만든 RDD에 스키마를 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# SparkContext 객체 생성\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# txt file 읽어오기\n",
    "lines = sc.textFile('./people.txt')\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "\n",
    "## Step 1 ## => value들 처리\n",
    "# 각 라인을 tuple( , ) 형태로 convert 해주기 \n",
    "people = parts.map(lambda p: (p[0], p[1].strip())) # name에서 공백 strip\n",
    "\n",
    "## Step 2 ## => Schema들 처리\n",
    "# 문자열로 인코딩된 스키마\n",
    "schemaString = \"name age\"\n",
    "# schemaString 요소를 loop돌면서 StructField로 만들기\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "# StructField 여러개가 있는 리스트를 StrucType으로 만들기!\n",
    "schema = StructType(fields)\n",
    "\n",
    "## Step 3 ## => value와 schema 활용해 DataFrame 생성\n",
    "# 위에서 만든 schema를 RDD의 schema로 적용\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# View Table 생성해 쿼리 날려서 데이터 추출해보기\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``count(), countDistince(), avg(), max(), min() 등``과 같은 집계 함수도 존재\n",
    "- 사용자가 직접 집계함수 customizing 할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 연습\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path = '/Users/younghun/Desktop/gitrepo/TIL/pyspark/people.json'\n",
    "peopleDF = spark.read.json(path)\n",
    "peopleDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      30|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.createOrReplaceTempView('people')\n",
    "res = spark.sql('SELECT MAX(age) FROM people')\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
