{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.6196284\n",
      "200 0.4959296\n",
      "400 0.46423128\n",
      "600 0.44172403\n",
      "800 0.42360285\n",
      "1000 0.40788603\n",
      "1200 0.39367494\n",
      "1400 0.38052878\n",
      "1600 0.3682153\n",
      "1800 0.3566017\n",
      "2000 0.34560576\n",
      "2200 0.3351713\n",
      "2400 0.32525605\n",
      "2600 0.31582555\n",
      "2800 0.30685005\n",
      "3000 0.2983028\n",
      "3200 0.29015923\n",
      "3400 0.28239617\n",
      "3600 0.27499223\n",
      "3800 0.26792696\n",
      "4000 0.2611812\n",
      "4200 0.2547368\n",
      "4400 0.24857678\n",
      "4600 0.24268515\n",
      "4800 0.2370468\n",
      "5000 0.23164766\n",
      "5200 0.2264743\n",
      "5400 0.22151439\n",
      "5600 0.21675615\n",
      "5800 0.2121886\n",
      "6000 0.20780164\n",
      "6200 0.20358543\n",
      "6400 0.199531\n",
      "6600 0.19562979\n",
      "6800 0.19187395\n",
      "7000 0.18825598\n",
      "7200 0.1847689\n",
      "7400 0.1814061\n",
      "7600 0.1781614\n",
      "7800 0.17502908\n",
      "8000 0.17200369\n",
      "8200 0.16907994\n",
      "8400 0.16625308\n",
      "8600 0.1635186\n",
      "8800 0.16087224\n",
      "9000 0.15830997\n",
      "9200 0.15582782\n",
      "9400 0.15342243\n",
      "9600 0.15109032\n",
      "9800 0.1488282\n",
      "10000 0.14663315\n",
      "\n",
      "Hypothesis:\n",
      " [[0.02954526]\n",
      " [0.15719463]\n",
      " [0.29909858]\n",
      " [0.78402984]\n",
      " [0.9412316 ]\n",
      " [0.9807263 ]] \n",
      "Correct(Y):\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# sigmoid함수를 input summation 해주는 활성함수 적용\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cross entropy라는 Cost function정의\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1- hypothesis))\n",
    "# SGD 적용해서 minimize cost\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# 0.5값 이상이면 True(1)로 예측되도록 함\n",
    "# cast함수: float형으로 출력되는 hypothesis값이 0.5보다 크면 정수 1(True), 작으면 정수 0(False)으로 변환\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "# 예측값과 실제값 비교해서 Boolean으로 반환 후, 1(True)값들의 평균을 내어서 accuracy 출력\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),\n",
    "                                 dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train],\n",
    "                              feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "    \n",
    "    # 위에서 역전파 학습으로 cost를 최소하하도록 학습한 후 마지막 업데이트 상태에서 forwarding한 후 예측된 확률값, 예측된 클래스값, 정확도 출력\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis:\\n\", h, \"\\nCorrect(Y):\\n\", c,\"\\nAccuracy:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 당뇨병 데이터 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/younghun/Desktop/20-2_bigdata/deep_learning/deeplearning_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.57945496\n",
      "200 0.5653891\n",
      "400 0.5579338\n",
      "600 0.5525767\n",
      "800 0.54800695\n",
      "1000 0.5438531\n",
      "1200 0.54000235\n",
      "1400 0.53641087\n",
      "1600 0.5330542\n",
      "1800 0.5299139\n",
      "2000 0.5269738\n",
      "2200 0.5242194\n",
      "2400 0.5216372\n",
      "2600 0.51921487\n",
      "2800 0.516941\n",
      "3000 0.5148049\n",
      "3200 0.51279694\n",
      "3400 0.51090807\n",
      "3600 0.50912994\n",
      "3800 0.5074549\n",
      "4000 0.50587577\n",
      "4200 0.5043859\n",
      "4400 0.50297946\n",
      "4600 0.5016507\n",
      "4800 0.5003944\n",
      "5000 0.49920583\n",
      "5200 0.49808046\n",
      "5400 0.49701425\n",
      "5600 0.49600327\n",
      "5800 0.49504417\n",
      "6000 0.4941336\n",
      "6200 0.49326843\n",
      "6400 0.492446\n",
      "6600 0.49166363\n",
      "6800 0.4909188\n",
      "7000 0.49020937\n",
      "7200 0.48953325\n",
      "7400 0.48888838\n",
      "7600 0.48827305\n",
      "7800 0.4876855\n",
      "8000 0.48712426\n",
      "8200 0.4865877\n",
      "8400 0.48607457\n",
      "8600 0.48558345\n",
      "8800 0.48511332\n",
      "9000 0.48466304\n",
      "9200 0.4842314\n",
      "9400 0.48381755\n",
      "9600 0.48342055\n",
      "9800 0.4830395\n",
      "10000 0.48267376\n",
      "\n",
      "Hypothesis:\n",
      " [[0.3322703 ]\n",
      " [0.9244751 ]\n",
      " [0.15262505]\n",
      " [0.9674512 ]\n",
      " [0.05052805]\n",
      " [0.778998  ]\n",
      " [0.9599619 ]\n",
      " [0.5945027 ]\n",
      " [0.219075  ]\n",
      " [0.5757507 ]\n",
      " [0.7438177 ]\n",
      " [0.12832373]\n",
      " [0.13824639]\n",
      " [0.13949645]\n",
      " [0.7120453 ]\n",
      " [0.47162235]\n",
      " [0.74260587]\n",
      " [0.89475596]\n",
      " [0.8496224 ]\n",
      " [0.5860937 ]\n",
      " [0.6839996 ]\n",
      " [0.08601391]\n",
      " [0.6405276 ]\n",
      " [0.7374259 ]\n",
      " [0.35741305]\n",
      " [0.9437624 ]\n",
      " [0.62362313]\n",
      " [0.66596097]\n",
      " [0.6643526 ]\n",
      " [0.37466133]\n",
      " [0.965459  ]\n",
      " [0.898062  ]\n",
      " [0.5274446 ]\n",
      " [0.79353946]\n",
      " [0.3259886 ]\n",
      " [0.58403045]\n",
      " [0.80701935]\n",
      " [0.45704222]\n",
      " [0.49973568]\n",
      " [0.3052953 ]\n",
      " [0.83896506]\n",
      " [0.13633385]\n",
      " [0.37818363]\n",
      " [0.01466233]\n",
      " [0.51427484]\n",
      " [0.91909695]\n",
      " [0.6740792 ]\n",
      " [0.70028365]\n",
      " [0.95149803]\n",
      " [0.9436029 ]\n",
      " [0.95458865]\n",
      " [0.22650966]\n",
      " [0.28472626]\n",
      " [0.9639512 ]\n",
      " [0.19699481]\n",
      " [0.5004224 ]\n",
      " [0.04897445]\n",
      " [0.7725481 ]\n",
      " [0.88799894]\n",
      " [0.4971198 ]\n",
      " [0.9485991 ]\n",
      " [0.70355594]\n",
      " [0.66729856]\n",
      " [0.85364187]\n",
      " [0.49429968]\n",
      " [0.5418432 ]\n",
      " [0.9709885 ]\n",
      " [0.7719495 ]\n",
      " [0.849377  ]\n",
      " [0.6990401 ]\n",
      " [0.22397879]\n",
      " [0.7414721 ]\n",
      " [0.9156581 ]\n",
      " [0.922153  ]\n",
      " [0.89260113]\n",
      " [0.7716616 ]\n",
      " [0.38078403]\n",
      " [0.8505807 ]\n",
      " [0.87807   ]\n",
      " [0.93372333]\n",
      " [0.88349074]\n",
      " [0.80691236]\n",
      " [0.4042238 ]\n",
      " [0.82728916]\n",
      " [0.55064946]\n",
      " [0.9074595 ]\n",
      " [0.48165697]\n",
      " [0.8969314 ]\n",
      " [0.9525353 ]\n",
      " [0.78812015]\n",
      " [0.88339883]\n",
      " [0.69243646]\n",
      " [0.7453984 ]\n",
      " [0.6078645 ]\n",
      " [0.90664387]\n",
      " [0.9833529 ]\n",
      " [0.9163993 ]\n",
      " [0.68200487]\n",
      " [0.14213282]\n",
      " [0.6502284 ]\n",
      " [0.66618985]\n",
      " [0.975316  ]\n",
      " [0.7015896 ]\n",
      " [0.7566206 ]\n",
      " [0.92524314]\n",
      " [0.72776127]\n",
      " [0.9492842 ]\n",
      " [0.87805176]\n",
      " [0.5550114 ]\n",
      " [0.23864955]\n",
      " [0.9636147 ]\n",
      " [0.8729428 ]\n",
      " [0.39453414]\n",
      " [0.44257239]\n",
      " [0.65110546]\n",
      " [0.7969675 ]\n",
      " [0.8348594 ]\n",
      " [0.95471334]\n",
      " [0.13518035]\n",
      " [0.70698607]\n",
      " [0.8888296 ]\n",
      " [0.68545955]\n",
      " [0.62427604]\n",
      " [0.8268857 ]\n",
      " [0.73300064]\n",
      " [0.88381505]\n",
      " [0.8596654 ]\n",
      " [0.58970195]\n",
      " [0.50383604]\n",
      " [0.25870207]\n",
      " [0.43375668]\n",
      " [0.7486223 ]\n",
      " [0.9441769 ]\n",
      " [0.8733981 ]\n",
      " [0.8274413 ]\n",
      " [0.86297977]\n",
      " [0.37927067]\n",
      " [0.81804013]\n",
      " [0.7761024 ]\n",
      " [0.72407925]\n",
      " [0.90872014]\n",
      " [0.6323068 ]\n",
      " [0.5621006 ]\n",
      " [0.6428374 ]\n",
      " [0.9271151 ]\n",
      " [0.67397535]\n",
      " [0.4910597 ]\n",
      " [0.93494135]\n",
      " [0.68674815]\n",
      " [0.7780126 ]\n",
      " [0.19040662]\n",
      " [0.29313833]\n",
      " [0.10441712]\n",
      " [0.21797106]\n",
      " [0.92255867]\n",
      " [0.86140406]\n",
      " [0.96404415]\n",
      " [0.09059072]\n",
      " [0.5209104 ]\n",
      " [0.85195625]\n",
      " [0.640408  ]\n",
      " [0.8608842 ]\n",
      " [0.35082716]\n",
      " [0.79980576]\n",
      " [0.5624865 ]\n",
      " [0.5824276 ]\n",
      " [0.70562696]\n",
      " [0.8937315 ]\n",
      " [0.80112374]\n",
      " [0.6226886 ]\n",
      " [0.8381808 ]\n",
      " [0.90383464]\n",
      " [0.9680418 ]\n",
      " [0.2075358 ]\n",
      " [0.83229   ]\n",
      " [0.31354868]\n",
      " [0.4048377 ]\n",
      " [0.30198592]\n",
      " [0.91047704]\n",
      " [0.63231546]\n",
      " [0.9565164 ]\n",
      " [0.91692996]\n",
      " [0.614213  ]\n",
      " [0.07889065]\n",
      " [0.1270557 ]\n",
      " [0.5904981 ]\n",
      " [0.7651886 ]\n",
      " [0.6811092 ]\n",
      " [0.8644173 ]\n",
      " [0.69251686]\n",
      " [0.31897467]\n",
      " [0.15150827]\n",
      " [0.8983511 ]\n",
      " [0.41167837]\n",
      " [0.89216757]\n",
      " [0.9044522 ]\n",
      " [0.7157247 ]\n",
      " [0.64341   ]\n",
      " [0.49416348]\n",
      " [0.5875478 ]\n",
      " [0.5895612 ]\n",
      " [0.96643764]\n",
      " [0.816502  ]\n",
      " [0.7881596 ]\n",
      " [0.1042437 ]\n",
      " [0.36669648]\n",
      " [0.9408094 ]\n",
      " [0.17114067]\n",
      " [0.9298247 ]\n",
      " [0.249084  ]\n",
      " [0.25949544]\n",
      " [0.46326855]\n",
      " [0.76208067]\n",
      " [0.15724435]\n",
      " [0.7821908 ]\n",
      " [0.7516007 ]\n",
      " [0.67805713]\n",
      " [0.6801783 ]\n",
      " [0.06554231]\n",
      " [0.3152829 ]\n",
      " [0.7187061 ]\n",
      " [0.5096766 ]\n",
      " [0.9339323 ]\n",
      " [0.9669162 ]\n",
      " [0.7081853 ]\n",
      " [0.25863594]\n",
      " [0.00472197]\n",
      " [0.7510359 ]\n",
      " [0.296238  ]\n",
      " [0.46385515]\n",
      " [0.9654926 ]\n",
      " [0.61896753]\n",
      " [0.9688731 ]\n",
      " [0.1808604 ]\n",
      " [0.08171579]\n",
      " [0.1672129 ]\n",
      " [0.73166966]\n",
      " [0.92270416]\n",
      " [0.9101124 ]\n",
      " [0.6117296 ]\n",
      " [0.5193845 ]\n",
      " [0.6350621 ]\n",
      " [0.06399691]\n",
      " [0.5472661 ]\n",
      " [0.06872472]\n",
      " [0.5412213 ]\n",
      " [0.8742976 ]\n",
      " [0.6436885 ]\n",
      " [0.7277799 ]\n",
      " [0.96884954]\n",
      " [0.7983017 ]\n",
      " [0.6693959 ]\n",
      " [0.73880565]\n",
      " [0.72309494]\n",
      " [0.8617015 ]\n",
      " [0.26053604]\n",
      " [0.42386925]\n",
      " [0.4241726 ]\n",
      " [0.7732886 ]\n",
      " [0.65302825]\n",
      " [0.6954902 ]\n",
      " [0.80007315]\n",
      " [0.20498648]\n",
      " [0.33572906]\n",
      " [0.44737858]\n",
      " [0.6265601 ]\n",
      " [0.27397594]\n",
      " [0.94842434]\n",
      " [0.79431343]\n",
      " [0.96496576]\n",
      " [0.5580496 ]\n",
      " [0.8257998 ]\n",
      " [0.7854843 ]\n",
      " [0.83478916]\n",
      " [0.6428504 ]\n",
      " [0.8048605 ]\n",
      " [0.31239986]\n",
      " [0.63858664]\n",
      " [0.72707456]\n",
      " [0.3853143 ]\n",
      " [0.8480097 ]\n",
      " [0.20685202]\n",
      " [0.5991448 ]\n",
      " [0.9535122 ]\n",
      " [0.8571336 ]\n",
      " [0.9035488 ]\n",
      " [0.68036973]\n",
      " [0.40249017]\n",
      " [0.6454524 ]\n",
      " [0.39512825]\n",
      " [0.43116367]\n",
      " [0.64179736]\n",
      " [0.6200806 ]\n",
      " [0.6617377 ]\n",
      " [0.55742097]\n",
      " [0.12728038]\n",
      " [0.70900273]\n",
      " [0.9530542 ]\n",
      " [0.52700496]\n",
      " [0.6562428 ]\n",
      " [0.83753085]\n",
      " [0.49635693]\n",
      " [0.7688653 ]\n",
      " [0.3310892 ]\n",
      " [0.6629271 ]\n",
      " [0.8979192 ]\n",
      " [0.66350454]\n",
      " [0.7250469 ]\n",
      " [0.85394037]\n",
      " [0.39835918]\n",
      " [0.88977766]\n",
      " [0.9618399 ]\n",
      " [0.27716547]\n",
      " [0.8504302 ]\n",
      " [0.26706782]\n",
      " [0.7441443 ]\n",
      " [0.81150717]\n",
      " [0.6560334 ]\n",
      " [0.3747833 ]\n",
      " [0.8132459 ]\n",
      " [0.78873515]\n",
      " [0.750195  ]\n",
      " [0.16244563]\n",
      " [0.8851944 ]\n",
      " [0.90821016]\n",
      " [0.33714405]\n",
      " [0.962571  ]\n",
      " [0.23924261]\n",
      " [0.7280977 ]\n",
      " [0.96580726]\n",
      " [0.2194682 ]\n",
      " [0.35726127]\n",
      " [0.7009891 ]\n",
      " [0.2720565 ]\n",
      " [0.16072246]\n",
      " [0.8597378 ]\n",
      " [0.9330182 ]\n",
      " [0.8492129 ]\n",
      " [0.6378865 ]\n",
      " [0.66389906]\n",
      " [0.66343814]\n",
      " [0.7561455 ]\n",
      " [0.82800853]\n",
      " [0.9536442 ]\n",
      " [0.7300328 ]\n",
      " [0.790599  ]\n",
      " [0.60431623]\n",
      " [0.9462411 ]\n",
      " [0.9507617 ]\n",
      " [0.7365036 ]\n",
      " [0.2677678 ]\n",
      " [0.64606494]\n",
      " [0.2530617 ]\n",
      " [0.82808006]\n",
      " [0.15324953]\n",
      " [0.18849504]\n",
      " [0.45090088]\n",
      " [0.7424029 ]\n",
      " [0.36808467]\n",
      " [0.54860777]\n",
      " [0.8654932 ]\n",
      " [0.6388982 ]\n",
      " [0.83951384]\n",
      " [0.9661474 ]\n",
      " [0.8394904 ]\n",
      " [0.02406687]\n",
      " [0.3049697 ]\n",
      " [0.86873627]\n",
      " [0.9066773 ]\n",
      " [0.6616653 ]\n",
      " [0.26342255]\n",
      " [0.9079535 ]\n",
      " [0.92311394]\n",
      " [0.28865007]\n",
      " [0.625836  ]\n",
      " [0.8519739 ]\n",
      " [0.8252783 ]\n",
      " [0.8390335 ]\n",
      " [0.87933   ]\n",
      " [0.9108443 ]\n",
      " [0.9426365 ]\n",
      " [0.6054103 ]\n",
      " [0.63561535]\n",
      " [0.5626607 ]\n",
      " [0.826707  ]\n",
      " [0.90072465]\n",
      " [0.20515978]\n",
      " [0.784794  ]\n",
      " [0.89556694]\n",
      " [0.25018144]\n",
      " [0.4037641 ]\n",
      " [0.86330247]\n",
      " [0.5185837 ]\n",
      " [0.91971684]\n",
      " [0.22518304]\n",
      " [0.85508883]\n",
      " [0.7215872 ]\n",
      " [0.8742173 ]\n",
      " [0.3633287 ]\n",
      " [0.7007568 ]\n",
      " [0.7336156 ]\n",
      " [0.78152156]\n",
      " [0.04800838]\n",
      " [0.13746902]\n",
      " [0.6298028 ]\n",
      " [0.83844674]\n",
      " [0.33245814]\n",
      " [0.8570105 ]\n",
      " [0.4980668 ]\n",
      " [0.31081843]\n",
      " [0.74645615]\n",
      " [0.39151797]\n",
      " [0.918169  ]\n",
      " [0.8630886 ]\n",
      " [0.72191983]\n",
      " [0.9384798 ]\n",
      " [0.73633885]\n",
      " [0.776479  ]\n",
      " [0.31952888]\n",
      " [0.29030722]\n",
      " [0.7482073 ]\n",
      " [0.45775148]\n",
      " [0.54847324]\n",
      " [0.931857  ]\n",
      " [0.9148253 ]\n",
      " [0.92932343]\n",
      " [0.96090263]\n",
      " [0.7194085 ]\n",
      " [0.80841625]\n",
      " [0.34071207]\n",
      " [0.32899827]\n",
      " [0.44672787]\n",
      " [0.94829726]\n",
      " [0.5472958 ]\n",
      " [0.13198116]\n",
      " [0.94706726]\n",
      " [0.85544765]\n",
      " [0.46504545]\n",
      " [0.75208634]\n",
      " [0.00722966]\n",
      " [0.92965096]\n",
      " [0.83737147]\n",
      " [0.78884214]\n",
      " [0.80617136]\n",
      " [0.972997  ]\n",
      " [0.59740394]\n",
      " [0.790164  ]\n",
      " [0.70455647]\n",
      " [0.87481874]\n",
      " [0.14081475]\n",
      " [0.59159994]\n",
      " [0.9349049 ]\n",
      " [0.700839  ]\n",
      " [0.76056445]\n",
      " [0.94868207]\n",
      " [0.87779945]\n",
      " [0.8882813 ]\n",
      " [0.41426572]\n",
      " [0.7977593 ]\n",
      " [0.954719  ]\n",
      " [0.7660381 ]\n",
      " [0.6508437 ]\n",
      " [0.32186818]\n",
      " [0.41055542]\n",
      " [0.49873206]\n",
      " [0.5571904 ]\n",
      " [0.56507766]\n",
      " [0.79841065]\n",
      " [0.57006735]\n",
      " [0.80298316]\n",
      " [0.84737074]\n",
      " [0.8235493 ]\n",
      " [0.6492945 ]\n",
      " [0.47273308]\n",
      " [0.5824279 ]\n",
      " [0.95572007]\n",
      " [0.8858465 ]\n",
      " [0.18623072]\n",
      " [0.4368079 ]\n",
      " [0.4433175 ]\n",
      " [0.0640583 ]\n",
      " [0.8751713 ]\n",
      " [0.1247375 ]\n",
      " [0.91862905]\n",
      " [0.8898465 ]\n",
      " [0.8477786 ]\n",
      " [0.61682343]\n",
      " [0.9043392 ]\n",
      " [0.36724037]\n",
      " [0.7833533 ]\n",
      " [0.9509665 ]\n",
      " [0.36669508]\n",
      " [0.3878646 ]\n",
      " [0.9207902 ]\n",
      " [0.87594974]\n",
      " [0.58716094]\n",
      " [0.8215499 ]\n",
      " [0.8322138 ]\n",
      " [0.82396835]\n",
      " [0.31118828]\n",
      " [0.7639519 ]\n",
      " [0.89952755]\n",
      " [0.636713  ]\n",
      " [0.7787355 ]\n",
      " [0.73063725]\n",
      " [0.8394383 ]\n",
      " [0.85708046]\n",
      " [0.95241606]\n",
      " [0.64569706]\n",
      " [0.40656924]\n",
      " [0.7843553 ]\n",
      " [0.72916704]\n",
      " [0.98245645]\n",
      " [0.82237846]\n",
      " [0.7015134 ]\n",
      " [0.3454144 ]\n",
      " [0.7287052 ]\n",
      " [0.9145926 ]\n",
      " [0.9689894 ]\n",
      " [0.9257546 ]\n",
      " [0.69471455]\n",
      " [0.5964011 ]\n",
      " [0.7978722 ]\n",
      " [0.3947986 ]\n",
      " [0.8444936 ]\n",
      " [0.78164434]\n",
      " [0.8777865 ]\n",
      " [0.5964105 ]\n",
      " [0.74469197]\n",
      " [0.8890165 ]\n",
      " [0.43994468]\n",
      " [0.536931  ]\n",
      " [0.66527605]\n",
      " [0.7294985 ]\n",
      " [0.58072346]\n",
      " [0.9437809 ]\n",
      " [0.95117736]\n",
      " [0.20504418]\n",
      " [0.09503296]\n",
      " [0.8013821 ]\n",
      " [0.64694977]\n",
      " [0.18079835]\n",
      " [0.85350996]\n",
      " [0.92083097]\n",
      " [0.7491728 ]\n",
      " [0.9550327 ]\n",
      " [0.9338796 ]\n",
      " [0.8073212 ]\n",
      " [0.85348666]\n",
      " [0.727971  ]\n",
      " [0.5612327 ]\n",
      " [0.7685567 ]\n",
      " [0.65683407]\n",
      " [0.08885458]\n",
      " [0.93403935]\n",
      " [0.8999973 ]\n",
      " [0.6816454 ]\n",
      " [0.9147577 ]\n",
      " [0.91763955]\n",
      " [0.92769265]\n",
      " [0.6389855 ]\n",
      " [0.7374417 ]\n",
      " [0.9038886 ]\n",
      " [0.77730924]\n",
      " [0.8896803 ]\n",
      " [0.93179244]\n",
      " [0.50667226]\n",
      " [0.8653799 ]\n",
      " [0.80083233]\n",
      " [0.5210442 ]\n",
      " [0.51339746]\n",
      " [0.10900605]\n",
      " [0.24739307]\n",
      " [0.81674206]\n",
      " [0.60572016]\n",
      " [0.7036538 ]\n",
      " [0.4031029 ]\n",
      " [0.91928214]\n",
      " [0.4480999 ]\n",
      " [0.8207051 ]\n",
      " [0.21703759]\n",
      " [0.8961503 ]\n",
      " [0.23961228]\n",
      " [0.85760486]\n",
      " [0.5829808 ]\n",
      " [0.7646736 ]\n",
      " [0.5705915 ]\n",
      " [0.17640015]\n",
      " [0.82106626]\n",
      " [0.9507836 ]\n",
      " [0.41105643]\n",
      " [0.92855823]\n",
      " [0.85759157]\n",
      " [0.8628079 ]\n",
      " [0.8308762 ]\n",
      " [0.42475027]\n",
      " [0.30388132]\n",
      " [0.6594793 ]\n",
      " [0.10916811]\n",
      " [0.96469486]\n",
      " [0.35182267]\n",
      " [0.9411628 ]\n",
      " [0.89766943]\n",
      " [0.37680504]\n",
      " [0.16154036]\n",
      " [0.583246  ]\n",
      " [0.43548837]\n",
      " [0.85392016]\n",
      " [0.745786  ]\n",
      " [0.9873699 ]\n",
      " [0.36327246]\n",
      " [0.6321596 ]\n",
      " [0.8524145 ]\n",
      " [0.59953123]\n",
      " [0.03388387]\n",
      " [0.80168164]\n",
      " [0.84209245]\n",
      " [0.90750283]\n",
      " [0.65358907]\n",
      " [0.45647454]\n",
      " [0.64548117]\n",
      " [0.9112451 ]\n",
      " [0.5595861 ]\n",
      " [0.84242976]\n",
      " [0.8160497 ]\n",
      " [0.89411014]\n",
      " [0.81076443]\n",
      " [0.527234  ]\n",
      " [0.8276276 ]\n",
      " [0.9139863 ]\n",
      " [0.70019466]\n",
      " [0.9741993 ]\n",
      " [0.7961757 ]\n",
      " [0.61378694]\n",
      " [0.5216912 ]\n",
      " [0.81009156]\n",
      " [0.85360366]\n",
      " [0.46964702]\n",
      " [0.6747041 ]\n",
      " [0.21217611]\n",
      " [0.6367016 ]\n",
      " [0.7907537 ]\n",
      " [0.9643178 ]\n",
      " [0.857877  ]\n",
      " [0.806169  ]\n",
      " [0.7351615 ]\n",
      " [0.9240195 ]\n",
      " [0.40120235]\n",
      " [0.95737004]\n",
      " [0.53680843]\n",
      " [0.8262579 ]\n",
      " [0.32407558]\n",
      " [0.03619519]\n",
      " [0.35650703]\n",
      " [0.3698753 ]\n",
      " [0.69192386]\n",
      " [0.8909526 ]\n",
      " [0.59012365]\n",
      " [0.7301818 ]\n",
      " [0.83478385]\n",
      " [0.58693874]\n",
      " [0.35704327]\n",
      " [0.87646043]\n",
      " [0.93648016]\n",
      " [0.36108536]\n",
      " [0.6750886 ]\n",
      " [0.15610203]\n",
      " [0.40618688]\n",
      " [0.73709404]\n",
      " [0.7197419 ]\n",
      " [0.88577056]\n",
      " [0.9861784 ]\n",
      " [0.14822099]\n",
      " [0.77025664]\n",
      " [0.5800165 ]\n",
      " [0.42691514]\n",
      " [0.718652  ]\n",
      " [0.727377  ]\n",
      " [0.8848526 ]\n",
      " [0.69586885]\n",
      " [0.5884635 ]\n",
      " [0.57661426]\n",
      " [0.15917227]\n",
      " [0.66938174]\n",
      " [0.6029062 ]\n",
      " [0.9221407 ]\n",
      " [0.54780024]\n",
      " [0.61012226]\n",
      " [0.7883872 ]\n",
      " [0.71834683]\n",
      " [0.3662333 ]\n",
      " [0.768157  ]\n",
      " [0.5964802 ]\n",
      " [0.2396768 ]\n",
      " [0.6152829 ]\n",
      " [0.92223907]\n",
      " [0.85134304]\n",
      " [0.56878465]\n",
      " [0.7745779 ]\n",
      " [0.29400486]\n",
      " [0.85291004]\n",
      " [0.52510434]\n",
      " [0.8185071 ]\n",
      " [0.34087676]\n",
      " [0.63205636]\n",
      " [0.86482084]\n",
      " [0.09426638]\n",
      " [0.24707285]\n",
      " [0.7543566 ]\n",
      " [0.85382426]\n",
      " [0.7856809 ]\n",
      " [0.90488   ]\n",
      " [0.8440267 ]\n",
      " [0.7551811 ]\n",
      " [0.8081639 ]\n",
      " [0.815951  ]\n",
      " [0.71172434]\n",
      " [0.8177607 ]\n",
      " [0.4061246 ]\n",
      " [0.43632883]\n",
      " [0.8982756 ]\n",
      " [0.84759563]\n",
      " [0.64718336]\n",
      " [0.2960996 ]\n",
      " [0.8930388 ]\n",
      " [0.82232785]\n",
      " [0.8241322 ]\n",
      " [0.71042407]\n",
      " [0.85697144]\n",
      " [0.88716686]\n",
      " [0.8017442 ]\n",
      " [0.40477753]\n",
      " [0.90743124]\n",
      " [0.932974  ]\n",
      " [0.27631497]\n",
      " [0.11135474]\n",
      " [0.7716259 ]\n",
      " [0.39280385]\n",
      " [0.8121449 ]\n",
      " [0.31352454]\n",
      " [0.45809448]\n",
      " [0.28554058]\n",
      " [0.8515376 ]\n",
      " [0.85939074]\n",
      " [0.11132014]\n",
      " [0.31808817]\n",
      " [0.6059918 ]\n",
      " [0.47987992]\n",
      " [0.5289395 ]\n",
      " [0.816429  ]\n",
      " [0.1671069 ]\n",
      " [0.92816854]\n",
      " [0.13970387]\n",
      " [0.86362994]\n",
      " [0.8144829 ]\n",
      " [0.6992445 ]\n",
      " [0.8687373 ]\n",
      " [0.7273591 ]\n",
      " [0.89400357]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:\n",
      " 0.76943344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',',\n",
    "               dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "# 주의: y값 shape를 2차원형태로 만들어주어야 함\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1- hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),\n",
    "                                 dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed = {X : x_data, Y : y_data}\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict=feed)\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "    \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict=feed)\n",
    "    print(\"\\nHypothesis:\\n\", h, \"\\nCorrect:\\n\", c, \"\\nAccuracy:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동물 multi class 분류 데이터 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',',\n",
    "               dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0\n",
      "Loss:4.3287\n",
      "Acc:0.13\n",
      "Step:100\n",
      "Loss:0.6182\n",
      "Acc:0.84\n",
      "Step:200\n",
      "Loss:0.3911\n",
      "Acc:0.90\n",
      "Step:300\n",
      "Loss:0.2986\n",
      "Acc:0.96\n",
      "Step:400\n",
      "Loss:0.2434\n",
      "Acc:0.96\n",
      "Step:500\n",
      "Loss:0.2056\n",
      "Acc:0.98\n",
      "Step:600\n",
      "Loss:0.1774\n",
      "Acc:0.98\n",
      "Step:700\n",
      "Loss:0.1555\n",
      "Acc:0.98\n",
      "Step:800\n",
      "Loss:0.1379\n",
      "Acc:0.98\n",
      "Step:900\n",
      "Loss:0.1235\n",
      "Acc:0.98\n",
      "Step:1000\n",
      "Loss:0.1116\n",
      "Acc:0.99\n",
      "Step:1100\n",
      "Loss:0.1016\n",
      "Acc:0.99\n",
      "Step:1200\n",
      "Loss:0.0932\n",
      "Acc:0.99\n",
      "Step:1300\n",
      "Loss:0.0861\n",
      "Acc:0.99\n",
      "Step:1400\n",
      "Loss:0.0800\n",
      "Acc:0.99\n",
      "Step:1500\n",
      "Loss:0.0747\n",
      "Acc:0.99\n",
      "Step:1600\n",
      "Loss:0.0701\n",
      "Acc:1.00\n",
      "Step:1700\n",
      "Loss:0.0661\n",
      "Acc:1.00\n",
      "Step:1800\n",
      "Loss:0.0625\n",
      "Acc:1.00\n",
      "Step:1900\n",
      "Loss:0.0592\n",
      "Acc:1.00\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:5, True_Y:5\n",
      "[True] Prediction:4, True_Y:4\n",
      "[True] Prediction:4, True_Y:4\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:5, True_Y:5\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:5, True_Y:5\n",
      "[True] Prediction:5, True_Y:5\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:5, True_Y:5\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:5, True_Y:5\n",
      "[True] Prediction:4, True_Y:4\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:2, True_Y:2\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:2, True_Y:2\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:2, True_Y:2\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:5, True_Y:5\n",
      "[True] Prediction:4, True_Y:4\n",
      "[True] Prediction:2, True_Y:2\n",
      "[True] Prediction:2, True_Y:2\n",
      "[True] Prediction:3, True_Y:3\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:1, True_Y:1\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:5, True_Y:5\n",
      "[True] Prediction:0, True_Y:0\n",
      "[True] Prediction:6, True_Y:6\n",
      "[True] Prediction:1, True_Y:1\n"
     ]
    }
   ],
   "source": [
    "# 클래스 개수 할당\n",
    "nb_classes = 7\n",
    "# 16개의 Feature\n",
    "X = tf.placeholder(tf.float32, shape=[None, 16])\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "# ?는 몇개의 데이터 개수가 들어올지 모르니 유동적인 값을 의미\n",
    "# target을 원 핫 인코딩 시켜주기, one_hot은 그런데 3차원 shape(?, 1, 7)로 반환함.. \n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
    "# 그래서 2차원 shape으로 만들어주어야 함. (?, 7)로!\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "# 활성함수를 Softmax function을 사용\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# 텐서플로우는 Softmax 활성함수와 cross-entropy Cost function을 한 번에 정의 가능. 대신 logits인자에 활성함수 적용 '전'을 넣어야 함!\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                labels=Y_one_hot)\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# argmax 인자의 1은 max를 찾을 방향을 찾는것: 0=행, 1=열\n",
    "# tf.argmax : hypothesis 배열 중 열(1)기준으로 가장 큰 값을 갖는 index를 반환시킴\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "# 예측값과 실제값을 비교해 Boolean값 맞췄으면 True(1), 틀렸으면 False(0)으로 반환\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "# 예측값과 실제값이 일치하는 True값들의 평균 계산하여 accuracy 계산\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 2000번 학습을 통해서 SGD수행\n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy],\n",
    "                                feed_dict={X:x_data, Y:y_data})\n",
    "            print(f\"Step:{step}\\nLoss:{loss:.4f}\\nAcc:{acc:.2f}\")\n",
    "            \n",
    "    # 2000번의 학습 종료 후 마지막에 업데이트된 파라미터값으로 예측값 출력\n",
    "    pred = sess.run(prediction, feed_dict={X:x_data})\n",
    "    # 실제값의 차원을 예측값과 일치시켜준 후 예측값과 실제값 각각 출력\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(f\"[{p==int(y)}] Prediction:{p}, True_Y:{int(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
