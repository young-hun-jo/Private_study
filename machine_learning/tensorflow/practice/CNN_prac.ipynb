{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-1adf6e33d929>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CNN\n",
    "def build_CNN_clf(x):\n",
    "    x_image = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    ## conv1 layer ##\n",
    "    # 필터 shape 설정(초기랜덤값 설정할 때 표준편차 설정)\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 1, 32],\n",
    "                                             stddev=5e-2))\n",
    "    # bias 설정\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "    # h_conv1\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1,\n",
    "                                     strides=[1, 1, 1, 1],\n",
    "                                     padding='SAME') + b_conv1)\n",
    "    # h_conv1을 max pooling\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "    \n",
    "    ## conv2 layer ##\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 32, 64],\n",
    "                                             stddev=5e-2))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2,\n",
    "                                     strides=[1, 1, 1, 1],\n",
    "                                     padding='SAME') + b_conv2)\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "    \n",
    "    ## Fully connected layer1 ##\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal(shape=[7*7*64, 1024],\n",
    "                                           stddev=5e-2))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "    # h_pool2를 flatten 시켜주기(풀링한 후 이미지 가로*세로*필터개수)\n",
    "    h_pool2_flat = tf.reshape(h_pool2, shape=[-1, 7*7*64])\n",
    "    # flatten시킨거랑 W_fc1 내적해서 활성함수 적용\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    ## Fully connected layer2 ## - 최종 예측값 출력시키는 layer\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal(shape=[1024, 10],\n",
    "                                           stddev=5e-2))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    # h_fc1과 W_fc2 내적해서 softmax(활성함수) 사용해 최종 예측값 출력\n",
    "    # logits과 예측값 개별 변수에 할당\n",
    "    logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "    \n",
    "    return logits, y_pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0번째 Loss:2.3827 Accuracy:0.1200\n",
      "Epoch 100번째 Loss:0.2711 Accuracy:0.9200\n",
      "Epoch 200번째 Loss:0.3266 Accuracy:0.8800\n",
      "Epoch 300번째 Loss:0.1676 Accuracy:0.9600\n",
      "Epoch 400번째 Loss:0.0634 Accuracy:0.9800\n",
      "Epoch 500번째 Loss:0.1043 Accuracy:0.9800\n",
      "Epoch 600번째 Loss:0.1886 Accuracy:0.9600\n",
      "Epoch 700번째 Loss:0.0819 Accuracy:0.9600\n",
      "Epoch 800번째 Loss:0.0362 Accuracy:1.0000\n",
      "Epoch 900번째 Loss:0.0254 Accuracy:1.0000\n",
      "Epoch 1000번째 Loss:0.0668 Accuracy:0.9800\n",
      "Test 데이터 검증 결과- Loss:0.1020 Accuracy:0.9665\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "logits, y_pred = build_CNN_clf(x)\n",
    "\n",
    "#loss function \n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,\n",
    "                                                 logits=logits)\n",
    "# SGD \n",
    "sgd = tf.train.AdamOptimizer(learning_rate=0.002).minimize(loss)\n",
    "\n",
    "# 예측값 vs 실제값 정답 여부 확인\n",
    "correct_pred = tf.cast(tf.equal(tf.arg_max(y_pred, 1), tf.arg_max(y, 1)),\n",
    "                      dtype=tf.float32)\n",
    "# 정확도 측정\n",
    "accuracy = tf.reduce_mean(correct_pred)\n",
    "\n",
    "# 만든 Tensor들 Run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1001):\n",
    "        # 데이터 50개씩 batch size 설정\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            # 100번 iteration 돌 때마다 loss값, accuracy값 출력\n",
    "            train_loss, train_acc = sess.run([loss, accuracy],\n",
    "                                            feed_dict={x:batch[0],\n",
    "                                                      y:batch[1]})\n",
    "            train_loss = np.mean(train_loss)\n",
    "            print(f\"Epoch {i}번째 Loss:{train_loss:.4f} Accuracy:{train_acc:.4f}\")\n",
    "        # SGD으로 loss값 최소화 시키면서 파라미터 업데이트\n",
    "        sess.run([sgd], feed_dict={x:batch[0], y:batch[1]})\n",
    "    \n",
    "    # 모든 학습이 끝나고 Test 데이터로 Loss, Accuracy 측정\n",
    "    test_loss, test_acc = sess.run([loss, accuracy],\n",
    "                                  feed_dict={x:mnist.test.images,\n",
    "                                            y:mnist.test.labels})\n",
    "    test_loss = np.mean(test_loss)\n",
    "    print(f\"Test 데이터 검증 결과- Loss:{test_loss:.4f} Accuracy:{test_acc:.4f}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# [batch_size, height, width, 컬러\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000 // 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN\n",
    "def build_CNN_clf(x):\n",
    "    # x가 애초에 4차원 shape=[데이터개수, 높이, 너비, 채널개수(컬러)]로 됨\n",
    "    x_image = x\n",
    "    \n",
    "    ### conv1 layer ### - 컬러니까 3!\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 64],\n",
    "                                             stddev=5e-2))\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1,\n",
    "                                     strides=[1, 1, 1, 1],\n",
    "                                     padding='SAME') + b_conv1)\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "    #### conv2 layer ###\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 128],\n",
    "                                             stddev=5e-2))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2,\n",
    "                                     strides=[1, 1, 1, 1],\n",
    "                                     padding='SAME') + b_conv2)\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 3, 3, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "    \n",
    "    ### conv3 layer ###\n",
    "    W_conv3 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 256],\n",
    "                                             stddev=5e-2))\n",
    "    b_conv3 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
    "    h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3,\n",
    "                                     strides=[1, 2, 2, 1],\n",
    "                                     padding='SAME'))\n",
    "    ### conv4 layer ###\n",
    "    W_conv4 = tf.Variable(tf.truncated_normal(shape=[3, 3, 256, 256]))\n",
    "    b_conv4 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
    "    h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4,\n",
    "                                     strides=[1, 2, 2, 1],\n",
    "                                     padding='SAME'))\n",
    "    \n",
    "    ### FC layer 1 ###\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal(shape=[8*8*256, 1024],\n",
    "                                           stddev=5e-2))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1 , shape=[1024]))\n",
    "    # h_conv4 flatten 시키기\n",
    "    h_conv4_flat = tf.reshape(h_conv4, shape=[-1, 8*8*256])\n",
    "    # flatten시킨 거랑 W_fc1 내적하고 활성함수 적용\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv4_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # Dropout 적용\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    ### FC layer 2 ### - 최종 예측값 출력하는 layer\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal(shape=[1024, 10],\n",
    "                                           stddev=5e-2))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1 , shape=[10]))\n",
    "    # 예측값 계산하는 logits\n",
    "    logits = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    # softmax로 예측값 분류\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "    \n",
    "    return logits, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    # 두개를 반환하는데, 함수 사용시 변수를 하나로 할당하면 튜플로 담김!\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32 ,3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "# unit(node) 얼마나 Dropout시킬지 placeholder 정의\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "# Y값 원-핫 인코딩 시켜놓기\n",
    "y_train_ohe = tf.squeeze(tf.one_hot(y_train, depth=10), axis=1)\n",
    "y_test_ohe = tf.squeeze(tf.one_hot(y_test, depth=10), axis=1)\n",
    "\n",
    "logits, y_pred = build_CNN_clf(x)\n",
    "\n",
    "#cost function \n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,\n",
    "                                                 logits=logits))\n",
    "# SGD\n",
    "sgd = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "# 예측값 vs 실제값 일치 체크\n",
    "correct_pred = tf.equal(tf.arg_max(y_pred, 1), tf.arg_max(y, 1))\n",
    "# 정확도 계산\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
    "\n",
    "# 만든 텐서 Run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1001):\n",
    "        # batch_size 설정위해 직접 정의한 함수 사용\n",
    "        # 텐서사용하지 않고 직접 정의한 함수이기 때문에 텐서로 만든 y_train_ohe는 eval사용해 객체로 할당\n",
    "        batch = next_batch(128, x_train, y_train_ohe.eval())\n",
    "        if i % 100 == 0:\n",
    "            train_loss, train_acc = sess.run([loss, accuracy],\n",
    "                                            feed_dict={x:batch[0],\n",
    "                                                      y:batch[1],\n",
    "                                                      keep_prob:1.0})\n",
    "            print(f\"Epoch {i}번째 Loss:{train_loss:.4f} Accuracy:{train_acc:.4f}\")\n",
    "            \n",
    "        # SGD수행\n",
    "        sess.run([sgd], feed_dict={x:batch[0], y:batch[1],\n",
    "                                  keep_prob:0.8})\n",
    "    # 학습 완료하고 테스트 데이터도 batch_size로 테스트\n",
    "    test_acc = 0.0\n",
    "    # 테스트 데이터가 만개이니까 1000개씩 10번 테스트해서 평균 정확도 계산\n",
    "    for i in range(10):\n",
    "        test_batch = next_batch(100, x_test, y_test_ohe.eval())\n",
    "        acc = accuracy.eval(feed_dict={x: test_batch[0],\n",
    "                                      y: test_batch[1],\n",
    "                                      keep_prob:1.0})\n",
    "        test_acc += acc\n",
    "    avg_test_acc = test_acc / 10\n",
    "    print()\n",
    "    print(f\"Test 데이터 평균 정확도: {avg_test_acc:.4f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(a, b):\n",
    "    return np.asarray(a), np.asarray(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prac = test([1,3,5], [2,4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
