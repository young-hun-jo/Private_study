{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스그룹 분류 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# subset인자에 all로 train, test데이터 모두 가져오기\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "# news_data는 dict형로되어 있음. 속성값들 확인해보기\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "종속변수 클래스 값과 분포도\n",
      " 10    999\n",
      "15    997\n",
      "8     996\n",
      "9     994\n",
      "11    991\n",
      "13    990\n",
      "7     990\n",
      "5     988\n",
      "14    987\n",
      "2     985\n",
      "12    984\n",
      "3     982\n",
      "6     975\n",
      "1     973\n",
      "4     963\n",
      "17    940\n",
      "16    910\n",
      "0     799\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "종속변수 클래스 이름들\n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# 클래스 값과 분포도 살펴보기\n",
    "import pandas as pd\n",
    "print('종속변수 클래스 값과 분포도\\n',\n",
    "     pd.Series(news_data.target).value_counts(ascending=False))\n",
    "print('종속변수 클래스 이름들\\n',\n",
    "     news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 하나만 살펴보기\n",
    "print(news_data.data[0])\n",
    "\n",
    "# 헤더값들에 텍스트의 주요 정보가 포함되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: 11314\n",
      " 테스트 데이터 크기: 7532\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train',\n",
    "                               remove=('headers','footers','quotes'),\n",
    "                               random_state=12)\n",
    "X_train = train_data.data\n",
    "y_train = train_data.target\n",
    "\n",
    "test_data = fetch_20newsgroups(subset='test',\n",
    "                              remove=('headers','footers','quotes'),\n",
    "                              random_state=12)\n",
    "X_test = test_data.data\n",
    "y_test = test_data.target\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(X_train)}\\n 테스트 데이터 크기: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 shape:(11314, 101631)\n",
      " 테스트 데이터 shape:(7532, 101631)\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터 feature Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vect = CountVectorizer()\n",
    "# CountVecorizer도 한번 fit 하면 검증, 테스트 데이터에도 똑같은 것으로 해야함!\n",
    "# 다시 한 번 fit하게 되면 벡터화되는 feature 개수가 달라짐!\n",
    "X_train_vec = cnt_vect.fit_transform(X_train)\n",
    "X_test_vec = cnt_vect.transform(X_test)\n",
    "\n",
    "# feature 벡터화시킨 후 shape 보기\n",
    "print(f\"학습 데이터 shape:{X_train_vec.shape}\\n 테스트 데이터 shape:{X_test_vec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류 정확도 :  0.6053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/younghun/opt/anaconda3/envs/venvforpython/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 리그레션으로 모델링하기\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_vec, y_train)\n",
    "y_pred = lr_clf.predict(X_test_vec)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"분류 정확도 : {acc : .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf 벡터화 후 정확도 : 0.6737\n"
     ]
    }
   ],
   "source": [
    "# Tf-idf로 feature vectorizer 시킨 후 로지스틱리그레션 해보기\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "X_train_tf = tf_idf.fit_transform(X_train)\n",
    "X_test_tf = tf_idf.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tf, y_train)\n",
    "y_pred = lr_clf.predict(X_test_tf)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Tf-idf 벡터화 후 정확도 :{acc : .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2gram, 불용어 제거 적용 후 정확도 :0.5150\n"
     ]
    }
   ],
   "source": [
    "# Vecorizer 파라미터 추가한 후 적용해보기\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tf_idf = TfidfVectorizer(ngram_range=(2,2), stop_words='english')\n",
    "X_train_tf = tf_idf.fit_transform(X_train)\n",
    "X_test_tf = tf_idf.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tf, y_train)\n",
    "y_pred = lr_clf.predict(X_test_tf)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"2gram, 불용어 제거 적용 후 정확도 :{acc :.4f}\")\n",
    "# ngram_range 2개이상으로 늘리면 성능 올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 C 파리미터 값 : {'C': 1}\n",
      "최적의 Estimator로 학습시킨 후 정확도 :0.5150\n"
     ]
    }
   ],
   "source": [
    "# 위에서 Tf-idf 벡터화 시킨 후 GridSearchCV 사용해보기\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 로지스틱 리그레션 규제 파라미터 C = 1/alpha임. 따라서 C값이 클수록 약한 규제!\n",
    "C_params = {'C':[0.01, 0.1, 1]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf, param_grid=C_params,\n",
    "                         cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_lr.fit(X_train_tf, y_train)\n",
    "print(\"최적의 C 파리미터 값 :\", grid_cv_lr.best_params_)\n",
    "\n",
    "y_pred = grid_cv_lr.predict(X_test_tf)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"최적의 Estimator로 학습시킨 후 정확도 :{acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``하단 2개의 모델은 시간이 오래걸려 동작 생략``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 벡터화와 모델링 한번에 Pipeline으로 사용하기\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Pipeline은 list안에 튜플('객체',과정) 식으로 선언\n",
    "pipeline = Pipeline([('tf_idf', TfidfVectorizer(ngram_range=(2,2),\n",
    "                                               stop_words='english'),\n",
    "                     'lr_clf', LogisticRegression(C=1))])\n",
    "# 파이프라인을 사용하면 벡토라이저 함수에서 fit_transform 없어도 자동으로 해줌\n",
    "# 따라서 파이프라인의 fit인자에 벡터화시키기 전 train, test 데이터를 넣어주자\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"파이프라인을 적용한 정확도 : {acc : .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번엔 feature 벡터화와 GridSearchCV 한번에 파이프라인을 이용해서 수행하기\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('tf_idf',TfidfVectorizer(stop_words='english')),\n",
    "                     ('lr_clf',LogisitcRegression())])\n",
    "# Pipeline 객체를 GridSearch 인자의 모델부분에 넣어주려면\n",
    "# params 할당할 때 Pipeline의 객체 이름 뒤에 '__파라미터이름'을 붙여줘서 인식하도록 해야함!\n",
    "params = {'tf_idf__ngram_range':[(1,1),(2,2),(3,3)],\n",
    "         'tf_idf__max_df':[100,200,300],\n",
    "         'lr_clf__C':[0.1,5,10]}\n",
    "\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params,\n",
    "                           cv=3, scoring='accuracy',verbose=1)\n",
    "# Pipeline때문에 벡터화시키기 전의 원본 데이터를 넣어줌\n",
    "grid_cv_pipe.fit(X_train y_train)\n",
    "print(\"최적의 파라미터 값\", grid_cv_pipe.best_params_)\n",
    "\n",
    "y_pred = grid_cv_pipe.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"파이프라인+그리드서치 적용 후 test데이터 정확도:{acc :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 분류기 주로 사용되는 모델 - **``로지스틱 리그레션, 나이브베이즈, SVM``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
