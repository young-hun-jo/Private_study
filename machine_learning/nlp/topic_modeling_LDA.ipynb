{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 newsgroup을 이용한 LDA 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 기반 벡터화 시킨 후 shape: (7862, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# LDA는 빈도수에만 기반하는 CountVectorizer사용함!\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 주어진 데이터셋의 일부 카테고리 데이터만 추출하므로 카테고리 사전에 설정\n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
    "        'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med'  ]\n",
    "# 설정해준 카테고리의 데이터들만 추출\n",
    "news_df = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'),\n",
    "                            categories=cats, random_state=12)\n",
    "# CountVectorizer로 텍스트 데이터들 단어 빈도수에 기반해 벡터화시키기(fit_transform까지!)\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000,\n",
    "                            min_df=2, stop_words='english',\n",
    "                            ngram_range=(1,2))\n",
    "ftr_vect = count_vect.fit_transform(news_df.data)\n",
    "print('Count 기반 벡터화 시킨 후 shape:',ftr_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lda.fit 까지만 하면 토픽별로 단어들의 분포를 수치로 알려줌!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=8, random_state=42)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA클래스를 이용해서 피처 벡터화시킨 것을 토픽모델링 시키기\n",
    "# 8개의 주제만 뽑았으니 n_components(토픽개수) 8로 설정\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=42)\n",
    "lda.fit(ftr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.63519060e+01, 1.86054803e+01, 5.84604286e+00, ...,\n",
       "        1.92702247e+02, 1.25083617e-01, 6.13626785e+00],\n",
       "       [2.04214200e+02, 3.50284932e+02, 1.31948001e+02, ...,\n",
       "        1.09520222e+01, 2.24437656e+02, 1.96644739e+01],\n",
       "       [4.54593156e+01, 9.30356531e+01, 2.25020590e+01, ...,\n",
       "        5.03797203e-01, 5.49831722e+01, 4.63608380e+01],\n",
       "       ...,\n",
       "       [1.36793837e+02, 1.87036321e+01, 1.26271967e-01, ...,\n",
       "        1.33888270e+01, 1.59536601e+01, 3.43179992e+00],\n",
       "       [7.16218683e-01, 4.49780560e+00, 9.14720569e+00, ...,\n",
       "        2.24127048e+01, 1.25116055e-01, 1.50499552e+01],\n",
       "       [1.25075332e-01, 1.25062790e-01, 1.25002714e-01, ...,\n",
       "        1.29818061e+02, 1.25117101e-01, 4.65081589e+01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# components_속성은 8개의 토픽별(row)로 1000개의 feature(단어)들의 분포수치(column)를 보여줌\n",
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽별 단어 분포 확인하기\n",
    "\n",
    "- fit까지 하면 -> 토픽별 단어들의 분포를 알려줌\n",
    "- 각 토픽별로 가장 중심이 되는 단어들이 무엇인지 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 때 lda_model이란, 벡터화시킨 텍스트 데이터를 fit까지만 적용한 모델!\n",
    "def display_topic_words(lda_model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        print('\\nTopic #', topic_idx+1)\n",
    "        \n",
    "        # Topic별로 1000개의 단어들(features)중에서 높은 값 순으로 정렬 후 index를 반환해줌!\n",
    "        # argsort()는 디폴트가 오름차순임(1,2,3,...) 그래서 [::-1]로 내림차순으로 바꿔주기\n",
    "        topic_word_idx = topic.argsort()[::-1]\n",
    "        top_idx = topic_word_idx[:num_top_words]\n",
    "        \n",
    "        # CountVectorizer함수 할당시킨 객체에 get_feature_names()로 벡터화시킨 feature(단어들)볼 수 있음!\n",
    "        # 이 벡터화시킨 단어들(features)은 숫자-알파벳순으로 정렬되며, 단어들 순서는 fit_transform시키고 난 이후에도 동일!\n",
    "        # '문자열'.join 함수로 특정 문자열 사이에 끼고 문자열 합쳐줄 수 있음.\n",
    "        feature_concat = '+'.join([str(feature_names[i])+'*'+str(round(topic[i], 1)) for i in top_idx])\n",
    "        print(feature_concat)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic # 1\n",
      "like*1423.2+just*1260.0+don*1243.5+know*1010.5+good*904.8+ve*852.7+think*822.8+use*748.4+time*705.2+does*656.3+make*579.1+want*542.5+really*541.4+bike*540.1+used*534.4\n",
      "\n",
      "Topic # 2\n",
      "armenian*937.0+turkish*686.1+armenians*677.0+jews*607.1+people*581.4+government*451.3+turkey*394.0+jewish*386.3+war*376.0+000*350.3+armenia*340.6+muslim*322.8+genocide*321.1+turks*320.1+new*319.4\n",
      "\n",
      "Topic # 3\n",
      "10*568.9+medical*444.8+1993*399.9+12*387.8+health*379.0+research*356.8+20*335.1+disease*327.3+cancer*321.1+patients*301.7+11*293.1+92*286.4+information*275.5+april*258.9+number*253.3\n",
      "\n",
      "Topic # 4\n",
      "said*756.4+don*694.0+year*692.7+just*634.3+know*605.1+didn*574.9+time*562.3+people*547.1+like*487.9+game*477.6+think*460.8+went*450.0+did*449.8+say*435.5+going*387.8\n",
      "\n",
      "Topic # 5\n",
      "file*1123.1+jpeg*782.6+program*755.3+use*694.6+window*568.8+does*538.1+image*535.4+output*527.1+color*517.6+display*511.5+files*450.8+gif*434.8+thanks*420.2+entry*389.6+bit*364.9\n",
      "\n",
      "Topic # 6\n",
      "edu*1605.4+graphics*1014.4+software*772.7+available*768.9+image*759.5+com*755.8+ftp*737.6+dos*677.5+mail*638.4+data*608.5+pub*534.7+windows*494.7+information*439.3+version*436.0+list*421.6\n",
      "\n",
      "Topic # 7\n",
      "israel*670.6+israeli*449.9+widget*390.5+motif*332.6+mit*311.2+sun*304.0+x11*291.5+subject*280.7+arab*248.9+set*243.4+window*230.1+openwindows*220.4+usr*217.1+export*203.8+x11r5*199.7\n",
      "\n",
      "Topic # 8\n",
      "god*2007.0+people*999.7+jesus*690.1+church*661.4+does*651.5+think*642.1+believe*638.9+say*568.3+christ*550.0+know*519.0+christian*480.8+don*471.8+did*466.3+just*443.8+christians*440.8\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vect.get_feature_names()\n",
    "display_topic_words(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 결과값들에서 단어들의 분포도(높은 순으로 정렬한 상태)를 보고 어떤 주제일지 결정하는 것은 **``사람의 몫``** 이다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서별 토픽 분포 확인하기\n",
    "\n",
    "- fit되어 있는 LDA모델에서 transform까지 수행한 후!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7862, 8)\n",
      "[[0.70540011 0.00543799 0.00543866 0.00544077 0.26195794 0.00544178\n",
      "  0.00544319 0.00543957]\n",
      " [0.01564333 0.01563743 0.64749634 0.01563402 0.01563859 0.01567394\n",
      "  0.01565302 0.25862333]]\n"
     ]
    }
   ],
   "source": [
    "# transform까지 수행하면, 문서별(row)로 토픽들(column)의 분포를 알려줌\n",
    "doc_topics = lda.transform(ftr_vect)\n",
    "print(doc_topics.shape)\n",
    "print(doc_topics[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 내장 텍스트데이터의 문서이름에는 카테고리가 labeling되어있음. \n",
    "# 따라서, 카테고리가 무엇인지 아는 상태이니까 어떤 문서들이 어떤 토픽들이 높은지 확인해보자.\n",
    "# 그리고 그 토픽들이 각각 무엇을 내용으로 하는지 추측해보자.\n",
    "# 주어진 데이터셋의 filename속성을 이용해서 카테고리값들 가져오기\n",
    "def get_filename_list(newsdata):\n",
    "    filename_lst = []\n",
    "    for file in newsdata.filenames:\n",
    "        filename_temp = file.split('/')[-2:]\n",
    "        filename = '.'.join(filename_temp)\n",
    "        filename_lst.append(filename)\n",
    "    return filename_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7862\n"
     ]
    }
   ],
   "source": [
    "filename_lst = get_filename_list(news_df)\n",
    "print(len(filename_lst))\n",
    "# 7862개의 문서들이 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic #0</th>\n",
       "      <th>Topic #1</th>\n",
       "      <th>Topic #2</th>\n",
       "      <th>Topic #3</th>\n",
       "      <th>Topic #4</th>\n",
       "      <th>Topic #5</th>\n",
       "      <th>Topic #6</th>\n",
       "      <th>Topic #7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>comp.graphics.38765</th>\n",
       "      <td>0.705400</td>\n",
       "      <td>0.005438</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.005440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med.59107</th>\n",
       "      <td>0.015643</td>\n",
       "      <td>0.015637</td>\n",
       "      <td>0.647496</td>\n",
       "      <td>0.015634</td>\n",
       "      <td>0.015639</td>\n",
       "      <td>0.015674</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.258623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics.54182</th>\n",
       "      <td>0.529201</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.362516</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.081073</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>0.005444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles.103182</th>\n",
       "      <td>0.672086</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.199593</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>0.091545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian.21740</th>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.987828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball.105077</th>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.996020</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast.75974</th>\n",
       "      <td>0.319845</td>\n",
       "      <td>0.453337</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.215440</td>\n",
       "      <td>0.002278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast.76050</th>\n",
       "      <td>0.300003</td>\n",
       "      <td>0.173406</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.294612</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>0.181921</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.012522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian.20900</th>\n",
       "      <td>0.890520</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>0.015628</td>\n",
       "      <td>0.015662</td>\n",
       "      <td>0.015637</td>\n",
       "      <td>0.015629</td>\n",
       "      <td>0.015629</td>\n",
       "      <td>0.015658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics.54334</th>\n",
       "      <td>0.960181</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.005689</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>0.005694</td>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.005689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball.102625</th>\n",
       "      <td>0.282534</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0.427938</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>0.007815</td>\n",
       "      <td>0.250426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics.53888</th>\n",
       "      <td>0.576820</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.013895</td>\n",
       "      <td>0.013911</td>\n",
       "      <td>0.339772</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>0.013909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles.104346</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian.21612</th>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.370388</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.052510</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.571972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics.53969</th>\n",
       "      <td>0.827290</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>0.144881</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.004638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics.53503</th>\n",
       "      <td>0.013914</td>\n",
       "      <td>0.013891</td>\n",
       "      <td>0.013902</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.595642</td>\n",
       "      <td>0.320960</td>\n",
       "      <td>0.013893</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles.104518</th>\n",
       "      <td>0.020878</td>\n",
       "      <td>0.020856</td>\n",
       "      <td>0.020838</td>\n",
       "      <td>0.671246</td>\n",
       "      <td>0.203636</td>\n",
       "      <td>0.020849</td>\n",
       "      <td>0.020835</td>\n",
       "      <td>0.020863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med.59148</th>\n",
       "      <td>0.009639</td>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.333106</td>\n",
       "      <td>0.009631</td>\n",
       "      <td>0.009626</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.609136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med.59044</th>\n",
       "      <td>0.167925</td>\n",
       "      <td>0.030677</td>\n",
       "      <td>0.765205</td>\n",
       "      <td>0.032854</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics.38385</th>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.294151</td>\n",
       "      <td>0.666344</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.006585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Topic #0  Topic #1  Topic #2  Topic #3  \\\n",
       "comp.graphics.38765           0.705400  0.005438  0.005439  0.005441   \n",
       "sci.med.59107                 0.015643  0.015637  0.647496  0.015634   \n",
       "sci.electronics.54182         0.529201  0.005439  0.005444  0.362516   \n",
       "rec.motorcycles.103182        0.672086  0.007357  0.007353  0.199593   \n",
       "soc.religion.christian.21740  0.001740  0.001738  0.001741  0.001739   \n",
       "rec.sport.baseball.105077     0.000568  0.996020  0.000569  0.000569   \n",
       "talk.politics.mideast.75974   0.319845  0.453337  0.002274  0.002277   \n",
       "talk.politics.mideast.76050   0.300003  0.173406  0.012516  0.294612   \n",
       "soc.religion.christian.20900  0.890520  0.015638  0.015628  0.015662   \n",
       "sci.electronics.54334         0.960181  0.005685  0.005689  0.005686   \n",
       "rec.sport.baseball.102625     0.282534  0.007824  0.007830  0.427938   \n",
       "sci.electronics.53888         0.576820  0.013890  0.013903  0.013895   \n",
       "rec.motorcycles.104346        0.125000  0.125000  0.125000  0.125000   \n",
       "soc.religion.christian.21612  0.001027  0.001026  0.370388  0.001027   \n",
       "sci.electronics.53969         0.827290  0.004644  0.004646  0.004636   \n",
       "sci.electronics.53503         0.013914  0.013891  0.013902  0.013898   \n",
       "rec.motorcycles.104518        0.020878  0.020856  0.020838  0.671246   \n",
       "sci.med.59148                 0.009639  0.009620  0.333106  0.009631   \n",
       "sci.med.59044                 0.167925  0.030677  0.765205  0.032854   \n",
       "comp.graphics.38385           0.006592  0.006582  0.006584  0.006581   \n",
       "\n",
       "                              Topic #4  Topic #5  Topic #6  Topic #7  \n",
       "comp.graphics.38765           0.261958  0.005442  0.005443  0.005440  \n",
       "sci.med.59107                 0.015639  0.015674  0.015653  0.258623  \n",
       "sci.electronics.54182         0.005441  0.081073  0.005442  0.005444  \n",
       "rec.motorcycles.103182        0.007355  0.007355  0.007355  0.091545  \n",
       "soc.religion.christian.21740  0.001739  0.001738  0.001739  0.987828  \n",
       "rec.sport.baseball.105077     0.000568  0.000569  0.000568  0.000568  \n",
       "talk.politics.mideast.75974   0.002274  0.002275  0.215440  0.002278  \n",
       "talk.politics.mideast.76050   0.012512  0.181921  0.012508  0.012522  \n",
       "soc.religion.christian.20900  0.015637  0.015629  0.015629  0.015658  \n",
       "sci.electronics.54334         0.005692  0.005694  0.005686  0.005689  \n",
       "rec.sport.baseball.102625     0.007817  0.007814  0.007815  0.250426  \n",
       "sci.electronics.53888         0.013911  0.339772  0.013901  0.013909  \n",
       "rec.motorcycles.104346        0.125000  0.125000  0.125000  0.125000  \n",
       "soc.religion.christian.21612  0.001025  0.052510  0.001025  0.571972  \n",
       "sci.electronics.53969         0.144881  0.004634  0.004632  0.004638  \n",
       "sci.electronics.53503         0.595642  0.320960  0.013893  0.013900  \n",
       "rec.motorcycles.104518        0.203636  0.020849  0.020835  0.020863  \n",
       "sci.med.59148                 0.009626  0.009617  0.009625  0.609136  \n",
       "sci.med.59044                 0.000835  0.000835  0.000835  0.000835  \n",
       "comp.graphics.38385           0.294151  0.666344  0.006581  0.006585  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe형태로 만들어보기\n",
    "import pandas as pd\n",
    "topic_names = ['Topic #'+ str(i) for i in range(0,8)]\n",
    "topic_df = pd.DataFrame(data=doc_topics, columns=topic_names,\n",
    "                       index=filename_lst)\n",
    "topic_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫 번째 행의 문서는 원래 ``컴퓨터 그래픽``에 관한 문서이다. 이 문서는 Topic 0번이랑 Topic 4번이 주를 이룬다. \n",
    "\n",
    "\n",
    "- 하지만 실제 LDA 모델은 위와 같은 상황이 주어지는 것보다 ``Topic별 단어들의 분포를 보고 -> 각 Topic을 'A','B','C'...이다 라고 사람이 추론하고 -> 각 문서별 Topic들의 분포 수치를 본 후 -> 각 문서가 이런 A,B 라는 토픽들이 주를 이루니까 그 문서는 어떤 문서겠구나! 추론하는 과정이다!!``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
