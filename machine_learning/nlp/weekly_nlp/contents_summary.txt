<Week6>
## Topic : 주제(document=text) 분류(classification) ##

- 연속적인 값을 예측하는 Linear regression에 Sigmoid함수를 씌우게 되면 Logistic regression인 분류문제해결로 변경 가능!

-근데 input을 텍스트로 어떻게 넣을까? 이 때 우리가 앞서 배웠던 tf-idf , Bow 로 단어를 N차원 벡터로 만드는 것을 이용하자!(=vectorize)
=> 이렇게 N차원 벡터한개가 하나의 Feature(Dataframe형태에서 하나의 attribute column이라고 보면 될듯!?)

-텍스트를 vectorize해서 input으로 넣어주고 sigmoid함수를 거쳐 0~1사이의 값을 반환하는데, 보통 0.5를 기준으로 클래스를 나눔

-텍스트에서 어떤 단어들이 중요했는지 파악해볼 수 있다. feature importance라고 하는데 이는 곧 각 feature에게
할당된 각기 다른 수준의 weight값이라고 할 수 있다.

<Week7>은 생략
<Week8>
이번 글도 잘 보았습니다. 스팸메일 분류기가 나이브베이즈 분류기로 작동하는거였군요... 스팸 자동 분류기라는 것을 표면적으로 봤을 때는 어떠한 머신러닝 알고리즘을 이용하는 건지 감이오질 않았었는데 하나하나 뜯어봐주시니 확 와닿는 것 같습니다!

질문 몇개 남기겠습니다!
1. likelihood에 해당하는 P(d|c)의 d에 해당하는 것이 해당 이메일에 들어가 있는 여러개의 단어들의 joint probability로 계산된다고 하셨잖아요!? 그렇다면 단어들 중에서 특정 단어들이 '스팸에 해당하는 단어'라는 label(답)을 이미알고있는 상태에서 학습되며(supervised learning) 그렇다면 어떤 단어는 스팸에 해당하는 단어이고 어떤 단어는 스팸에 해당하지 않는 단어라는 학습용 데이터셋이 이미 존재해야 하는거죠? 그럼 이러한 데이터셋은 어떻게 구하게 되나요..? 그냥 오픈소스로 공개된 데이터셋을 가져와서 사용하는 건가요?

2. 만약 새로운 'A'라는 단어가 학습용 데이터셋에 존재하지 않는 경우가 발생하면 Gaussian Naive Bayes를 이용하게 되는 건가요?

(답변)
1. 네 학습용 데이터셋이 필요한 supervised learning 방법입니다. 주로 네이버나 구글 같은 서비스들은 자기들의 데이터셋을 구축해놓았겠죠. 주로 유저들이 스팸이라고 신고하는 것을 데이터로 활용하지 않을까 싶어요. 스팸 관련 오픈소스 데이터 셋이 있는지는 잘 모르겠네요. 혹시 찾으셨다면 여기에 댓글로 공유 부탁드려요!

2. Naive Bayes Classifier는 학습용 데이터셋에 존재하지 않으면 이를 고려하지 않습니다. 단어가 P( 'A' | spam )이나 P('A' | not_spam)이 0일텐데, 확률이 전부 곱해지기 때문에 처음 보는 단어가 있으면 무조건 0이 되겠죠? 근데 모르는 단어가 있다고 스팸이거나 아닐 확률이 0이라는건 말이 안되기 때문에 그냥 계산에서 빼버립니다.

<Week9>
(질문)2. 언급해주셨던 pretrained 임베딩들은 저희가 앞서서 배웠던 word2vec, glove 알고리즘들도 pretrain된 모델을 사용하는 알고리즘이라는 이야기인건가요!?
(답변) 2) word2vec과 glove는 embedding을 어떻게 학습하는지에 대한 방법이고, pretrained embedding은 각각의 방법으로 많은 데이터를 가지고 학습한 결과물이라고 생각하시면 될듯합니다! 이해가 되셨는지요.

<Week10>
<3줄요약>
-CNN의 한계 : input길이의 갭이 큰 문장들을 효율적으로 처리하기 어려움
-NLP에서 RNN의 input으로는 각 timestep의 단어가 N곱1 column vector로 표현되어 들어감.
-GRU는 LSTM에 비해 더 적은 GATE를 사용해 비슷학 목적을 달성. 따라서 속도면에서 GRU가 LSTM에 비해 유리하고 만약 LSTM과 성능이 비슷할시 LSTM의 대용으로 사용됨

<Week11은 생략>
<Week12>
* Overfitting 막는법
1.학습 데이터 늘리기
2.모델 구조 바꾸기
3.학습 시간 늘리기
* 데이터분할시 shuffle하자
* validation ,test데이터에 train데이터가 중복되어있는지 확인하기
* train, test 데이터 각각에서 class불균형 맞춰주기 regression에서는 평균값, 표준편차값 맞춰주기
* test데이터는 최대한 안보기..!

Q. "세 부분의 분포가 최대한 비슷하도록 맞춰주자" 내용에서 Regression을 다룰 때 평균값과 표준편차를 맞춰준다는 것은 real-value값들을 Min max Scaler 나 Robust Scaler 와 같이 Scaler과정(normalization이라고 할 수 있는 과정!?)을 거쳐야 한다는 말씀이신건가요!?
A.음 Scaler는 feature input X 값을 맞추는 것이구요. 여기서 말하는 것은 예측하려는 Y값의 분포(distribution)를 말하는 것입니다.
Q.아하! 그렇다면 검증 데이터의 y값들의 평균과 표준편차를 학습 데이터의 y값들의 평균과 표준편차에 맞춰 준다는 말씀이신건가요!?
하나만 더 여쭈어 보겠습니다! 그렇다면 분류문제에서 class 간의 분포를 맞춰준다는 것은 class imbalance를 해결해주는 것을 말씀하시는 거죠!?
A.class imbalance를 해결한다기보다는 최대한 비슷하게 만드는게 좋다는 이야기입니다. 예를 들어 학습데이터에는 고양이 사진이 거의 없는데, 평가 데이터는 고양이 사진이 대부분이라면 제대로 된 성능을 내기 힘들겠죠?

<Week13은생략>
<Week14>
- Unigram 각 단어의 빈도수를 계산함. 하지만 문장 속 단어들간의 순서를 고려하지 못하는 단점 존재..
- 그래서 2개 이상의 단어를 고려하는 bi-gram, n개를 고려하는 n-gram이 등장.
- bi-gram이상 적용시 문장의 시작과 끝인 토큰들도 같이 bi-gram 해줌으로써 문장의 시작과 끝에 등장할 단어들의 확률을 높여줌!
#학습 데이터가 모든 경우의 수를 커버하지 못하는 현상 = data sparsity =>input데이터가 복잡할수록 이런 문제가 발생

<Week15>
< ASR(자동음성인식=automatic speech recognition) >
- ASR은 Noisy Channel Model을 통해서 음성을 텍스트로 변환. 변환하는 과정에서 통계학 모델이 개입되는데 그것은 베이지안 이론!
(여기서 Noisy Channel Model은 해독(decoding)하는 과정을 거친다고 할 수 있는데, 해독이란, 발신자가 신호를 보냈을 때 최대한 소음(nosie)을 제거하고 원본이 어떤 내용인지 알아내는 과정이라 함)
- ASR모델은 베이지안 정리를 통해 두가지 모델로 분리
1)소리(acoustic)모델
2)언어모델
- ASR모델의 베이지안 정리
X가 input으로 들어갈 소리, W가 해독하여 나오는 텍스트라고 할 때, P(W|X) = arg_max P(X|W)P(W)가 됨.(이때 P(X|W)란, W라는 텍스트가 주어졌을 때, 소리 X일확률)
여기서 P(X|W)는 소리모델, P(W)는 언어모델
위 두개의 모델 중 P(W)는 텍스트 W의 확률인데, 이것이 배웠던 LM(언어모델)

<Week16>

<글 요약>
* Phoenem(음운) dictionary : 영어 발음 기호처럼 된 음운을 단어와 mapping시켜 놓은 dictionary. ASR이 음성을 음운으로 변환하기 위해 만듦.
* WER(Word Error Rate) : ASR의 성능을 평가하는 지표. 만약 ASR = 0.9라면, 기계가 100개 중 10개의 단어만 이해하지 못하는 것을 뜻함.
[기존 ASR 방식에서 E2E ASR 방식으로 변환된 흐름]
1. AM(소리 모델)은 주어진 음성을 연속된 음운으로 변환함. 이 때 사용하는 머신러닝 모델은 HMM, DTW(Dynamic Time Wrapping)임.
2. 음운으로 변환후 LM(언어 모델)과 연계하여 인간이 알아볼 수 있는 단어들로 변환. 이 때 발음이 동일하지만 의미가 다른 단어들을 고려해야 해서 주변 문맥단어를 고려한 후(Ngram을 사용하겠죠..!? 맞지 않다면 지적해주세요..!) 적절한 단어로 변환
3. 1,2번과정을 거치는 AM+LM 결합방식인 기존의 ASR 방식은 최근 들어 E2E ASR 방식으로 변환. 이 때 E2E란, 여러개 문제 즉, AM+LM방식으로 하는 것이 아닌, 하나의 큰 모델을 사용하는 것인데, 큰 모델을 사용해 알아서 해주도록 하는 딥러닝(CNN, RNN) 방식으로 추세가 바뀜. 즉, E2E ASR은 '음성-텍스트' 데이터만 넣어주면 알아서 학습.
4. E2E ASR 방식은 딥러닝 학습방식(알아서 파라미터를 최적화)으로 인해 편리하지만 엄청난 대용량의 음성-텍스트 데이터가 필요하며 그만큼 비용(돈)도 많이 소모. 또한 모델 학습시간도 매우 긴 단점이 존재.

<Week 17> - 그동안 LM(언어 모델)내용 정리
- 언어 모델 (Language Modeling; LM)이란?: 어떤 문장이 주어졌을 때, 이 문장의 그럴듯함을 확률로 계산하는 모델
- LM은 어떻게 학습하는가: 큰 텍스트 데이터 셋(corpus)을 학습 데이터로 사용하여 통계 모델을 구축함.
- N-gram LM이란?: 데이터 셋에 나오는 단어 또는 연속되는 단어들의 빈도를 계산하여 LM을 만든 모델.
- LM은 무엇을 할 수 있는가? a) 여러 가지 문장이 주어졌을 때 가장 그럴듯한 문장을 뽑을 수 있음, b) 문장의 앞부분이.   주어졌을 때 다음 단어를 예측할 수 있음.
- N-gram의 첫 번째 한계는 BOW 한계와 마찬가지로 멀리 본 적이 없는 단어들을 예측 못함. 그래서 word2vec나 Glove와 같은 워드임베딩을 이용해서 뉴럴네트워크 모델을 통해 학습!
- N-gram의 두 번째 한계는 멀리 떨어져 있는 중요한 단어를 기억하지 못함. 그래서 RNN을 통해서 해결!

<Week 18>
이번 글도 잘 읽었습니다! 이번 글을 나름 이해하고 요약하는 데 꽤 걸렸네요...! 마지막에 질문 2가지 있습니다! 혹시 요약본에서 수정할 부분이 있다면 적극적인 지적은 환영입니다 :) 
<요약>
- N차원의 벡터인 Word embedding이 하나씩 RNN모델로 들어가 M차원의 벡터가 출력. 이를 Hidden state라고 함.
- 계속해서 여러 Word embedding이 들어가면서 결합 확률을 chain rule로 풀어서 쓸 수 있음.(이 때, 직전의 소수 단어들만 고려하는 것이 N-gram, 먼~ 단어까지 고려하는 것이 RNN)
- 이렇게 먼 단어까지 고려할 수 있는 이유는 Hidden state가 input으로 들어왔던 데이터들에 대한 기억을 갖고 있기 때문
- 이제 Hidden state 벡터들로 다음에 올 단어를 예측. 그러면 어떻게 RNN은 학습 데이터를 훈련시킬까?
- 학습 데이터를 가르칠 때 정답(label)이 존재해야 하는데 이 떄 정답의 역할로서 Vocabulary list를 만들어주어야 함. Vocabulary lists는 학습 데이터에 존재하는 단어들 중 빈도수가 높은 단어들로 이루어짐(One-hot vector처럼 이루어짐)
- 이 때, 학습 데이터들은 (input X, output Y)로 자유자재로 만들 수 있어서 따로 레이블링이 필요하지 않아 비지도 학습으로 분류되기도 함.(물론 LM에 한해서)
- 그래서 Hidden state가 Vocabulary list를 기반으로 다음 단어를 예측하는데, 이 때 활성함수를 Softmax function(multi-class classification 함수) 사용. 그리고 처음에는 Cost가 높을 것이기 때문에 이 Cost를 줄여주기 위해 Cost function으로 Cross entropy function 사용

Q2.  Hidden state에서 다음단어를 예측할 때 Vocabulary list를 사용하고 이 Vocabulary list는 단어 발생 여부를 담고 있는 One-hot vector로 이루어진다고 하셨습니다! 혹시 이 One-hot vector를 단어 발생 빈도 수를 담고 있는 Bag of Words 로 봐도 무방할까요..!? 여러 개념이 나오니 헷갈리네요 ㅜㅜ => 다음 단어를 예측하는 것은 '한 개'의 단어를 예측하는 것이고 이는 one-hot vector를 의미함!


<Week 19>
*LM의 평가 metric : Perplexity(공식 : 2의 entropy승), entropy가 지수로 들어가있기 때문에 Perplexity가 높을수록 entropy가 높다는 의미이며 결국 LM의 모델 성능이 좋지 않음을 의미

*Text Generation : LM이 글을 생성하는 것을 의미.
어떻게? LM이 확률분포가 매번 생성되는 Probabilistic model을 이용. 즉, Softmax function을 이용해 다음에 나올 단어들의 여러 후보에 확률 스코어를 만들어 주는데, 이를 계속적으로 샘플링 해서 글을 생성.
Ex) [Would you]라는 단어벡터가 들어가면 그 다음 나올 여러 후보 단어들 중에서 가장 확률이 높은 단어('have'라고 가정)가 나오고, 다시 input 단어벡터가 [Would you have]가 되고 또 이 문장 다음에 나올 여러 후보 단어들 중에서 가장 확률이 높은 단어를 선택... 이 과정을 계속적으로 반복하여 글을 생성.
