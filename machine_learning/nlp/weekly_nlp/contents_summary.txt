<Week6>
## Topic : 주제(document=text) 분류(classification) ##

- 연속적인 값을 예측하는 Linear regression에 Sigmoid함수를 씌우게 되면 Logistic regression인 분류문제해결로 변경 가능!

-근데 input을 텍스트로 어떻게 넣을까? 이 때 우리가 앞서 배웠던 tf-idf , Bow 로 단어를 N차원 벡터로 만드는 것을 이용하자!(=vectorize)
=> 이렇게 N차원 벡터한개가 하나의 Feature(Dataframe형태에서 하나의 attribute column이라고 보면 될듯!?)

-텍스트를 vectorize해서 input으로 넣어주고 sigmoid함수를 거쳐 0~1사이의 값을 반환하는데, 보통 0.5를 기준으로 클래스를 나눔

-텍스트에서 어떤 단어들이 중요했는지 파악해볼 수 있다. feature importance라고 하는데 이는 곧 각 feature에게
할당된 각기 다른 수준의 weight값이라고 할 수 있다.

<Week7>은 생략
<Week8>
이번 글도 잘 보았습니다. 스팸메일 분류기가 나이브베이즈 분류기로 작동하는거였군요... 스팸 자동 분류기라는 것을 표면적으로 봤을 때는 어떠한 머신러닝 알고리즘을 이용하는 건지 감이오질 않았었는데 하나하나 뜯어봐주시니 확 와닿는 것 같습니다!

질문 몇개 남기겠습니다!
1. likelihood에 해당하는 P(d|c)의 d에 해당하는 것이 해당 이메일에 들어가 있는 여러개의 단어들의 joint probability로 계산된다고 하셨잖아요!? 그렇다면 단어들 중에서 특정 단어들이 '스팸에 해당하는 단어'라는 label(답)을 이미알고있는 상태에서 학습되며(supervised learning) 그렇다면 어떤 단어는 스팸에 해당하는 단어이고 어떤 단어는 스팸에 해당하지 않는 단어라는 학습용 데이터셋이 이미 존재해야 하는거죠? 그럼 이러한 데이터셋은 어떻게 구하게 되나요..? 그냥 오픈소스로 공개된 데이터셋을 가져와서 사용하는 건가요?

2. 만약 새로운 'A'라는 단어가 학습용 데이터셋에 존재하지 않는 경우가 발생하면 Gaussian Naive Bayes를 이용하게 되는 건가요?

(답변)
1. 네 학습용 데이터셋이 필요한 supervised learning 방법입니다. 주로 네이버나 구글 같은 서비스들은 자기들의 데이터셋을 구축해놓았겠죠. 주로 유저들이 스팸이라고 신고하는 것을 데이터로 활용하지 않을까 싶어요. 스팸 관련 오픈소스 데이터 셋이 있는지는 잘 모르겠네요. 혹시 찾으셨다면 여기에 댓글로 공유 부탁드려요!

2. Naive Bayes Classifier는 학습용 데이터셋에 존재하지 않으면 이를 고려하지 않습니다. 단어가 P( 'A' | spam )이나 P('A' | not_spam)이 0일텐데, 확률이 전부 곱해지기 때문에 처음 보는 단어가 있으면 무조건 0이 되겠죠? 근데 모르는 단어가 있다고 스팸이거나 아닐 확률이 0이라는건 말이 안되기 때문에 그냥 계산에서 빼버립니다.

<Week9>
(질문)2. 언급해주셨던 pretrained 임베딩들은 저희가 앞서서 배웠던 word2vec, glove 알고리즘들도 pretrain된 모델을 사용하는 알고리즘이라는 이야기인건가요!?
(답변) 2) word2vec과 glove는 embedding을 어떻게 학습하는지에 대한 방법이고, pretrained embedding은 각각의 방법으로 많은 데이터를 가지고 학습한 결과물이라고 생각하시면 될듯합니다! 이해가 되셨는지요.

<Week10>
<3줄요약>
-CNN의 한계 : input길이의 갭이 큰 문장들을 효율적으로 처리하기 어려움
-NLP에서 RNN의 input으로는 각 timestep의 단어가 N곱1 column vector로 표현되어 들어감.
-GRU는 LSTM에 비해 더 적은 GATE를 사용해 비슷학 목적을 달성. 따라서 속도면에서 GRU가 LSTM에 비해 유리하고 만약 LSTM과 성능이 비슷할시 LSTM의 대용으로 사용됨

<Week11은 생략>
<Week12>
* Overfitting 막는법
1.학습 데이터 늘리기
2.모델 구조 바꾸기
3.학습 시간 늘리기
* 데이터분할시 shuffle하자
* validation ,test데이터에 train데이터가 중복되어있는지 확인하기
* train, test 데이터 각각에서 class불균형 맞춰주기 regression에서는 평균값, 표준편차값 맞춰주기
* test데이터는 최대한 안보기..!

Q. "세 부분의 분포가 최대한 비슷하도록 맞춰주자" 내용에서 Regression을 다룰 때 평균값과 표준편차를 맞춰준다는 것은 real-value값들을 Min max Scaler 나 Robust Scaler 와 같이 Scaler과정(normalization이라고 할 수 있는 과정!?)을 거쳐야 한다는 말씀이신건가요!?
A.음 Scaler는 feature input X 값을 맞추는 것이구요. 여기서 말하는 것은 예측하려는 Y값의 분포(distribution)를 말하는 것입니다.
Q.아하! 그렇다면 검증 데이터의 y값들의 평균과 표준편차를 학습 데이터의 y값들의 평균과 표준편차에 맞춰 준다는 말씀이신건가요!?
하나만 더 여쭈어 보겠습니다! 그렇다면 분류문제에서 class 간의 분포를 맞춰준다는 것은 class imbalance를 해결해주는 것을 말씀하시는 거죠!?
A.class imbalance를 해결한다기보다는 최대한 비슷하게 만드는게 좋다는 이야기입니다. 예를 들어 학습데이터에는 고양이 사진이 거의 없는데, 평가 데이터는 고양이 사진이 대부분이라면 제대로 된 성능을 내기 힘들겠죠?

<Week13은생략>
<Week14>
- Unigram 각 단어의 빈도수를 계산함. 하지만 문장 속 단어들간의 순서를 고려하지 못하는 단점 존재..
- 그래서 2개 이상의 단어를 고려하는 bi-gram, n개를 고려하는 n-gram이 등장.
- bi-gram이상 적용시 문장의 시작과 끝인 토큰들도 같이 bi-gram 해줌으로써 문장의 시작과 끝에 등장할 단어들의 확률을 높여줌!
#학습 데이터가 모든 경우의 수를 커버하지 못하는 현상 = data sparsity =>input데이터가 복잡할수록 이런 문제가 발생

<Week15>
< ASR(자동음성인식=automatic speech recognition) >
- ASR은 Noisy Channel Model을 통해서 음성을 텍스트로 변환. 변환하는 과정에서 통계학 모델이 개입되는데 그것은 베이지안 이론!
(여기서 Noisy Channel Model은 해독(decoding)하는 과정을 거친다고 할 수 있는데, 해독이란, 발신자가 신호를 보냈을 때 최대한 소음(nosie)을 제거하고 원본이 어떤 내용인지 알아내는 과정이라 함)
- ASR모델은 베이지안 정리를 통해 두가지 모델로 분리
1)소리(acoustic)모델
2)언어모델
- ASR모델의 베이지안 정리
X가 input으로 들어갈 소리, W가 해독하여 나오는 텍스트라고 할 때, P(W|X) = arg_max P(X|W)P(W)가 됨.(이때 P(X|W)란, W라는 텍스트가 주어졌을 때, 소리 X일확률)
여기서 P(X|W)는 소리모델, P(W)는 언어모델
위 두개의 모델 중 P(W)는 텍스트 W의 확률인데, 이것이 배웠던 LM(언어모델)

<Week16>

<글 요약>
* Phoenem(음운) dictionary : 영어 발음 기호처럼 된 음운을 단어와 mapping시켜 놓은 dictionary. ASR이 음성을 음운으로 변환하기 위해 만듦.
* WER(Word Error Rate) : ASR의 성능을 평가하는 지표. 만약 ASR = 0.9라면, 기계가 100개 중 10개의 단어만 이해하지 못하는 것을 뜻함.
[기존 ASR 방식에서 E2E ASR 방식으로 변환된 흐름]
1. AM(소리 모델)은 주어진 음성을 연속된 음운으로 변환함. 이 때 사용하는 머신러닝 모델은 HMM, DTW(Dynamic Time Wrapping)임.
2. 음운으로 변환후 LM(언어 모델)과 연계하여 인간이 알아볼 수 있는 단어들로 변환. 이 때 발음이 동일하지만 의미가 다른 단어들을 고려해야 해서 주변 문맥단어를 고려한 후(Ngram을 사용하겠죠..!? 맞지 않다면 지적해주세요..!) 적절한 단어로 변환
3. 1,2번과정을 거치는 AM+LM 결합방식인 기존의 ASR 방식은 최근 들어 E2E ASR 방식으로 변환. 이 때 E2E란, 여러개 문제 즉, AM+LM방식으로 하는 것이 아닌, 하나의 큰 모델을 사용하는 것인데, 큰 모델을 사용해 알아서 해주도록 하는 딥러닝(CNN, RNN) 방식으로 추세가 바뀜. 즉, E2E ASR은 '음성-텍스트' 데이터만 넣어주면 알아서 학습.
4. E2E ASR 방식은 딥러닝 학습방식(알아서 파라미터를 최적화)으로 인해 편리하지만 엄청난 대용량의 음성-텍스트 데이터가 필요하며 그만큼 비용(돈)도 많이 소모. 또한 모델 학습시간도 매우 긴 단점이 존재.

<Week 17> - 그동안 LM(언어 모델)내용 정리
- 언어 모델 (Language Modeling; LM)이란?: 어떤 문장이 주어졌을 때, 이 문장의 그럴듯함을 확률로 계산하는 모델
- LM은 어떻게 학습하는가: 큰 텍스트 데이터 셋(corpus)을 학습 데이터로 사용하여 통계 모델을 구축함.
- N-gram LM이란?: 데이터 셋에 나오는 단어 또는 연속되는 단어들의 빈도를 계산하여 LM을 만든 모델.
- LM은 무엇을 할 수 있는가? a) 여러 가지 문장이 주어졌을 때 가장 그럴듯한 문장을 뽑을 수 있음, b) 문장의 앞부분이.   주어졌을 때 다음 단어를 예측할 수 있음.
- N-gram의 첫 번째 한계는 BOW 한계와 마찬가지로 멀리 본 적이 없는 단어들을 예측 못함. 그래서 word2vec나 Glove와 같은 워드임베딩을 이용해서 뉴럴네트워크 모델을 통해 학습!
- N-gram의 두 번째 한계는 멀리 떨어져 있는 중요한 단어를 기억하지 못함. 그래서 RNN을 통해서 해결!

<Week 18>
이번 글도 잘 읽었습니다! 이번 글을 나름 이해하고 요약하는 데 꽤 걸렸네요...! 마지막에 질문 2가지 있습니다! 혹시 요약본에서 수정할 부분이 있다면 적극적인 지적은 환영입니다 :) 
<요약>
- N차원의 벡터인 Word embedding이 하나씩 RNN모델로 들어가 M차원의 벡터가 출력. 이를 Hidden state라고 함.
- 계속해서 여러 Word embedding이 들어가면서 결합 확률을 chain rule로 풀어서 쓸 수 있음.(이 때, 직전의 소수 단어들만 고려하는 것이 N-gram, 먼~ 단어까지 고려하는 것이 RNN)
- 이렇게 먼 단어까지 고려할 수 있는 이유는 Hidden state가 input으로 들어왔던 데이터들에 대한 기억을 갖고 있기 때문
- 이제 Hidden state 벡터들로 다음에 올 단어를 예측. 그러면 어떻게 RNN은 학습 데이터를 훈련시킬까?
- 학습 데이터를 가르칠 때 정답(label)이 존재해야 하는데 이 떄 정답의 역할로서 Vocabulary list를 만들어주어야 함. Vocabulary lists는 학습 데이터에 존재하는 단어들 중 빈도수가 높은 단어들로 이루어짐(One-hot vector처럼 이루어짐)
- 이 때, 학습 데이터들은 (input X, output Y)로 자유자재로 만들 수 있어서 따로 레이블링이 필요하지 않아 비지도 학습으로 분류되기도 함.(물론 LM에 한해서)
- 그래서 Hidden state가 Vocabulary list를 기반으로 다음 단어를 예측하는데, 이 때 활성함수를 Softmax function(multi-class classification 함수) 사용. 그리고 처음에는 Cost가 높을 것이기 때문에 이 Cost를 줄여주기 위해 Cost function으로 Cross entropy function 사용

Q2.  Hidden state에서 다음단어를 예측할 때 Vocabulary list를 사용하고 이 Vocabulary list는 단어 발생 여부를 담고 있는 One-hot vector로 이루어진다고 하셨습니다! 혹시 이 One-hot vector를 단어 발생 빈도 수를 담고 있는 Bag of Words 로 봐도 무방할까요..!? 여러 개념이 나오니 헷갈리네요 ㅜㅜ => 다음 단어를 예측하는 것은 '한 개'의 단어를 예측하는 것이고 이는 one-hot vector를 의미함!


<Week 19>
*LM의 평가 metric : Perplexity(공식 : 2의 entropy승), entropy가 지수로 들어가있기 때문에 Perplexity가 높을수록 entropy가 높다는 의미이며 결국 LM의 모델 성능이 좋지 않음을 의미

*Text Generation : LM이 글을 생성하는 것을 의미.
어떻게? LM이 확률분포가 매번 생성되는 Probabilistic model을 이용. 즉, Softmax function을 이용해 다음에 나올 단어들의 여러 후보에 확률 스코어를 만들어 주는데, 이를 계속적으로 샘플링 해서 글을 생성.
Ex) [Would you]라는 단어벡터가 들어가면 그 다음 나올 여러 후보 단어들 중에서 가장 확률이 높은 단어('have'라고 가정)가 나오고, 다시 input 단어벡터가 [Would you have]가 되고 또 이 문장 다음에 나올 여러 후보 단어들 중에서 가장 확률이 높은 단어를 선택... 이 과정을 계속적으로 반복하여 글을 생성.

<Week 20>
- MT(Machine Translation):기계번역
MT를 학습시키기 위한 데이터를 Parallel Corpora라고 함. 이것은 예를 들어 동일한 문장 의미의 한국어-영어 표현이 매핑된 데이터를 뜻함. 

- Phrase-based MT : 구절을 여러개로 나누어 각각을 번역하는데, 단계별로 번역을 접근
*1단계:각 구절,단어에 매핑되는 Dictionary(사전)을 만듦. 이 때, source-target 구절, 단어를 1:1 대응하기가 어렵기 때문에 특정 단어,구절이 여러번 반복되어 나오면 높은 확률로 저장하는 방식으로 사전을 만듦. 이것이 가능한 이유는 학습 데이터인 Parallel Corpora에서 target 단어,구절이 일종의 '정답'역할을 하기 때문!
*2단계:1단계에서 배운 사전을 사용해 문장을 구절로 나눔. 구절로 나눌 때 빈도수를 사용하게 된다면 특정 단어와 같이 자주 나오는 주변 단어 즉, n-gram 방식에 기반하여 하나의 구절로 묶어서 학습
*3단계:구절로 묶어 학습을 한 후 번역한 부분들을 순서에 맞게 잘 바꾸어주어야 함! 이를 Word alignment라 함. 이 때는 HMM 확률모델로 학습 후 Viterbi 알고리즘 방법으로 순서를 찾아냄.

위 단계에서 더 나아가면 어떤 부분이 주어,동사,목적어이고 관계가 어떻게 되는지 문장을 분석하는 것을 의미하는 Dependency parsing(의존구조분석)을 사용해서 구절 순서를 정렬. 이 때 단어가 늘어나면 복잡성이 매우 증가해서Dependency label(어떤것이 주어,목적어이고 관계가 이러하다)을 가지고 쉽고 효율적으로 구절을 올바른 순서로 정렬. 이것도 일종의 Dependency label이라 함은 어떤것이 주어, 목적어이고 관계가 이러하다 즉 '정답'을 아는 것!?

하지만 이것은 어색한 번역을 야기했고 neural MT에 의해 대체됨. Neural MT는 Dependency label이 없어도 올바르게 번역을 함. 추후 이 neural MT에 다룰 예정

Q1. Phrase-based MT의 1단계 방법론에서 Parallel corpora라는 데이터를 통해서 MT 모델이 통계학적인 접근을 해서 dictionary를 자동으로 생성한다고 하셨는데, 이 dictionary의 생김새는 어떻게 되는 건가요..? 시각적으로 감이 잘 안옵니다 ㅜㅜ

Q2. Dependency parsing 의 정의가 "어떤 부분이 주어고 동사고 관계가 어떻게 되는지를 분석하는 것"이라고 하시고 단어가 늘어날수록 발생하는 복잡성을 쉽고 효율적으로 만들기 위해 Dependency label를 가지고 번역한 구절의 순서를 맞춘다고 하셨습니다! 그렇다면 여기서 "Dependency label을 가지고 순서를 찾는다" 함은, "어떤것이 주어이고 동사고 관계가 이러하다"라는 것 즉, 일종의 '정답'을 알고있는 상태에서 순서를 찾는 건가요..? 마치 지도학습처럼..? 그렇다면 이 Dependency label은 사람이 직접 레이블링을 해주는 것인가요..? 어렵습니다 하하.. 

=> 답변
Q1 => 네, 그런 식으로 나옵니다. 실제 예시를 하나 넣어보았는데 이해가 되는지 한번 봐주세요!

Q2 => 네 맞습니다. 본문에 나온데로 별도의 머신러닝 모델이 쓰입니다. 이해하신데로 다른 모델의 예측을 일종의 '정답'이라고 생각하고 순서를 찾는거죠. dependency label 역시 지도학습으로 사람이 직접 레이블한 데이터로 학습됩니다. 나중에 깊게 다뤄보도록 하겠습니다!

<Week21>
* NMT(신경망 기계번역), seq 2 seq모델
# 번역도 "하나의 언어가 주어졌을 때 다른 언어로 변환되는 확률" 이라서 조건부 확률임. 이걸 학습시키기 위한 데이터는 parallel corpus
#ASR의 noisy channel model을 encoder-decoder 모델이라고 부르기도 함
#seq 2 seq 모델 = 조건부 LM + encoder-decoder 모델 + RNN 
- seq 2 seq 모델은 Encoder RNN 1개 + Decoder RNN 1개로 구성
- Decoder RNN에서 Encoder RNN의 마지막 hidden state를 계속적으로 참고함
Q.Decoder RNN에서 Encoder RNN의 마지막 Hidden state를 계속 참고한다고 하셨습니다! 
위 예시를 빌려보자면, love가 생성한 hidden state를 출력할 때 I가 생성한 hidden state를 참고해서 love의 hidden state를 출력하는 프로세스가 맞다면, Decoder RNN이 계속 참고하는 Encoder RNN의 마지막 hidden state에는 결국 Encoder RNN에 속한 모든 hidden state의 정보를 담고 있다! 라고 말하면 맞는 건가요!? 저의 사고가 맞는지 확인차 질문드립니다!

<Week22>
Seq2Seq의 Encoder RNN의 마지막 Hidden state는 N개의 실수로 이루어진 N by 1 열벡터로 도출. 이를 Sentence embedding이라고도 함.

이 sentence embedding을 참고해서 번역문장을 예측하는 Decoder RNN도 LM모델이 단어 하나씩 확률값을 계산(Softmax function)해 문장을 생성하는 프로세스와 동일. 하지만 일반 RNN LM과 다른 차이점 2개 존재

1.Decoder RNN의 input데이터에 sentence embedding도 같이 넣는다!
2.랜덤으로 샘플링하지 않고 확률이 높은 단어를 고른다!
즉, RNN LM과는 달리 Encoder RNN의 마지막 hidden state(sentence embedding)를 이어 받기 때문에 첫 단어를 랜덤샘플링이 아닌 가장 확률이 높은 단어를 고름.
그런데 이 첫 단어를 선택할 때 잘못 선택할 것을 방지하기 위해서 Top-K Beam Search가 존재
[Top-K Beam Search]
첫 단어로 올 후보 단어들을 K개 모두 input으로 집어넣은 후 다음 단계의 RNN으로 들어가서 다음 단어 후보들과의 조합을 통해 가장 적합한 조합들을 필터링(K는 하이퍼파라미터)

<Seq2Seq의 정답을 보고 틀린 부분 학습하는 방법>
Seq2Seq 모델도 RNN LM모델처럼 softmax layer이 생성한 가장 큰확률 점수 단어와 정답 단어와의 cross-entropy loss를 계산하면서 학습

But..Decoder로 오기 전 Encoder RNN에서 잘못 예측했다면? => BPTT(역전파)방법을 이용해서 encoder RNN에서 틀린 부분도 개선 가능!

<Week23>
-특정 도메인에서 자주 사용되는 단어를 번역하기 위해서는 전문 용어들이 많이 들어있는 corpus로 학습시키면 됨
-유머를 제대로 번역하는건 AI에게는 매우 고난이도..

-엄청 길어지는 문장들을 전부 기억하고 제대로 어떻게 번역할까?
=>앞서 배웠던 Encoder RNN의 마지막 하나의 hidden state(sentence embedding)가 Input문장에 대한
모든 정보를 기억하지만 input문장이 너무 길어지면 sentence embedding도 과부하가 걸림.. 그래서!

<Attention Mechanism등장!>
Encoder RNN의 마지막 hidden state(sentence embedding)만 이용하는 것이 아닌 encoder RNN에서 매 단어를 넣을 때마다 나오는 hidden state 전부를 이용하는 것
->그런데, 이 여러개의 hidden state 벡터들을 한 번에 묶는 방법이 필요. 이 방법이 바로 관심법(attention mechanism). 즉, 긴 문장에서 앞 부분을 번역시 앞 단어들에 좀 더 가중치를 두고 문장의 뒷 부분에서는 뒷 단어들에 더 가중치를 두는 것!
->그렇다면, 모델이 긴 문장의 여러 단어들 중에서 어떤 부분의 단어들에 더 가중치를 두고 안두고를 판단하게 될까? 이 또한 머신러닝을 통해 데이터를 학습해 단어끼리 어떻게 대응되는지 관계를 찾는 phrase alignment 학습

요즘 가장 핫한 Transformer, BERT 모델의 기반이 바로 이 Attention Mechanism!

Q.관심법에서는 매 단어를 넣을 때마다 나오는 모든 hidden state를 부분(구절)으로 나누어서 weighted sum 한다고 하셨습니다! 그렇다면 Decoder RNN의 첫 번째 단어를 예측할 때 참고하는 앞 부분의 Encoder RNN의 hidden state의 weighted sum한 형태는 일반적인 Seq 2 Seq모델의 sentence embedding처럼 N by 1의 열 벡터형태로 되어있는 건가요!?

<Week24> : 컴퓨터비전+RNN
이미지를 보고 기계가 글을 생성하는 image2text 기술[Image Captioning]
#하나의 이미지를 보고 설명할 수 있는 글이 정말 많지만, 생성하는 글 작성 기준도 '학습 데이터'에 기반

Seq 2 seq 모델의 encoder-decoder 모델에서 encoder가 '텍스트'에서 '이미지'로 변환된다는 점! 그래서 encoder에 CNN을 활용

여기서 encoder의 CNN도 Attention Mechanism(관심법)을 사용하는데, 이미지의 각각 다른 부분을 볼 때마다 하나의 단어를 내뱉는 것(연구 결과, 물체 이름 뿐만 아니라 영어 전치사의 공간적인 개념까지 학습하는 결과 나타남)

[Visual Question Answering/Generation-이미지를 보고 질문,답하기]
이제 image2text모델에게 질문을 던지면 답을 유추할 수 있게 하는 데이터셋이 등장해 모델이 이미지를 보고 질문을 생성하는 것도 가능

[Interactive Image Retrieval-모델이 두 이미지의 차이를 이해]

<Week 25>
<Attention 기반 Transformer>
-BERT, GPT의 기반이 되는 모델
-RNN은 "반바지를 입고 아이스크림을 들고 있던 아이는 그것을 금방 다 먹었다"에서 '그것'을 유추하기 위해 '먹었다'를 볼 수 없다!
- 이를 해결하기 위해 bi-directional RNN 등장(앞에서 가는 Forward RNN, 뒤에서오는 Backward RNN) - 이 아이디어는 ELMO 모델의 기반이 됨.
- 하지만 bi-directional RNN도 Backward의 RNN을 통해 '먹는다'는 볼 수 있지만 Backward RNN이 Forward RNN 부분을 동시에 참고할 수가 없음..
- 그래서 문장의 곳곳 어디든 참고할 수 있는 관심법 사용! 그런데 어디든 참고할 수 있는데 RNN이 굳이 필요한가? 그래서 컨베이어벨트같은 RNN을 없애버리고 워드 임베딩 벡터들만 남게 됨! 하지만 RNN처럼 단어를 순서대로 집어넣진 않지만 단어간의 순서는 고려해야 하기 때문에 Positional Embedding을 추가해줌!(sin, cos함수를 합성에 위치를 벡터로 표현..단어의 위치를 신호로 표현하는 것이 성능에 효과적)

-트랜스포머가 기존 관심법과 성능이 뛰어난 이유
1.Self-Attention:같은 문장안의 여러 단어 조합을 하나의 관심에 넣을 수 있음. 그래서 파라미터가 증가
2.Multi-headed Attention: '관심 위에 관심' 즉, 관심을 갖는 인코더를 여러개 쌓아올려서 단어 조합의 조합 또 그 조합의 조합까지 고려 가능
3.Parallelization: 문장 전체가 들어갈 때까지 기다리지 않고 각 위치에서 문장을 통채로 계산 가능. 이는 결국 분산 컴퓨팅이 가능하다는 것. 따라서 학습속도가 엄청 빨라짐

<Week 26> -BERT
[BERT]
# 관심법으로 문장 앞 뒤를 다 볼 수 있는 Transformer 모델을 기반으로 한 모델(내부 구조도 트랜스포머 모델과 동일)
# 간단한 프로세스 : 하나의 문장이 토큰화 -> word-embedding 벡터로 변환되어 입력 데이터로 들어감 -> 해당 입력에 대한 결과값이 나옴

# BERT는 어떻게 학습? 
=> MLM(Masked Language Model)
1단계) 주어진 문장에서 단어들의 15%정도를 마스킹하고 원래 단어가 무엇인지 예측(앞, 뒤, 중간 단어 아무거나 마스킹! 즉, 관심법을 활용해 앞, 뒤를 다 볼 수 있는 트랜스포머 특성을 이용함!)
2단계) 2개의 문장이 주어졌을 때 두 문장이 서로 관계가 있어 연이어 나올 문장인지 아니면 서로 독립적인 문장인지 예측

Q. BERT 가 문장 간의 관계를 학습할 때, 두 문장간의 관계가 있다없다 라는 이진분류로만 학습하는 것인가?

<Week 27> - Transformer learning(TL)
- 하나의 문제를 학습한 모델이 그 문제와 연관성이 있는 다른 문제를 좀 더 쉽게 해결한다!(문제의 유사성)
- representation learning: CNN의 필터함수들이 각기 패턴들을 학습하는 즉, 딥러닝 모델이 스스로 최종 문제 해결을 위해 중간 단계의 개념을 구조화 하는 것
-> 그런데 이렇게 학습시킨 필터함수들이 다른 이미지 처리 시에도 사용하면 유용하다는 것!

- 데이터가 적다면 무조건 TL을 사용하는 것이 좋다. 
- 즉, 이미 다른 데이터들로 학습된 모델(Pretrained model)을 사용하는 것이 TL이라 할 수 있다.
- 그래서 특정 문제를 해결하기 위한 모델링 시 pertained model이 있는지를 우선 확인해보자
- NLP분야에서 단어간의 관계를 학습해온 워드 임베딩과 LM에서 ELMO, BERT의 sentence-embedding이 TL이라 할 수 있다. 

<Week 28>
BERT로 풀 수 있는 문제 유형
1.문장 한개의 분류(스팸메일, 카테고리분류, 감성분석)
2.문장 두개 간의 관계 분류(두 문장의 의역 예측, 두 문장 주장간의 보완,상충, 중립 분류)
3. 문장 내 단어 라벨링(개체명 인식=이름이나 조직명의 시작/중간/끝 예측, POS 태깅 예측 문제)
4.묻고 답하기(BERT에게 질문과 (정답이 포함되어있는)본문을 던져주고 정답을 예측하는 문제)

BERT에 쓰이는 TL기법
1.Feature Extraction: 이미 학습된 BERT모델이 Output으로 내뱉는 단어벡터들을 내가 만들고자 하는 모델의 input으로 사용. 이 때 이미 학습된 BERT의 파라미터는 고정, 내가 만든 모델의 파라미터만 변경되면서 학습

2.Finetuning(재학습)-갖고있는 데이터가 더 많을 때 사용
1번과 달리 BERT모델의 파라미터도 (고정이 아닌)재학습을 시키는 방법. 
BERT모델의 파라미터도 재학습을 해야하기 때문에 미분이 가능한 활성함수나 그러한 비선형 활성함수를 갖는 모델을 내가 만들고자 하는 모델로 선정해야 함!

Q. 1번 기법에서는 BERT모델이 출력값으로 내뱉는 벡터들을 제가 만들고자 하는 모델의 입력데이터로 사용한다는 점은 이해가 확 됩니다! 그런데 2번 기법에서 다음과 같은 2가지 경우로 나뉜다고 했을 때,

첫째)BERT모델에 sigmoid같은 미분이 가능한 활성함수만을 붙였을 경우는 BERT가 [학습한 데이터+제가 갖고 있는 데이터]를 재학습시키고 최종 출력값을 도출하기 위한 마지막 활성함수(여기서 sigmoid)만 제가 덧붙인 셈이 되는 건가요?

둘째)BERT모델에 역전파가 가능한 모델을 붙였을 경우에는 1번 기법처럼 BERT가 내뱉은 벡터를 input으로 사용하는 것이 아니라 애초에 BERT모델이 제가 갖고 있는 데이터도 같이 학습시키면서 BERT안의 파라미터가 이미 대량 데이터로 학습되었을 때의 BERT안의 파라미터와는 다르게 업데이트 되는 건가요? 그렇다면 제가 새로 붙인 모델은 입력 데이터가 따로 입력되는 게 아닌가요!? 둘째)경우가 혼란스럽네요...ㅎㅎ

<Week 29> - GPT
[GPT:Generative Pretrained Transformer]
GPT도 RNN LM처럼 수많은 텍스트 데이터를 보고 통계모델을 이용해 다음 단어를 예측 -> 단어를 하나씩 예측하면서 이게 모여 문장, 더 나아가 글까지 생성

GPT가 성능이 좋은 가장 큰 이유는 방대한 양의 학습 데이터인 점
-G: GPT는 생성(Generative)모델인데,, 블로그 참고해보기
-P : 이미 학습된 모델. GPT는 하나의 Epoch로만 학습된 상태..왜냐하면 데이터가 너무 많아서..
-T : BERT가 사용한 트랜스포머의 부분과 다른 부분을 사용! 즉 인코더, 디코더 다 사용하는 BERT와는 달리 GPT는 미래의 단어를 예측하는 것이기 때문에 디코더 부분만 사용! => Q.그러면 GPT로는 번역 기능 불가..?

<Week 30> - GPT작동하는 원리
머신러닝의 한계 : 조금의 예제를 보고 새로운 풀 수 있을 정도로의 인간과 같은 지능은 못 넘어..(generalization ability 부족 문제)
- Few-shot learning: 몇개의 예제만 갖고 새로운 문제를 풀 수 있는 것
- Zero-shot learning: 예제를 아예 주지 않고 새로운 문제를 풀 수 있는 것

그런데, GPT가 이런 것을 할 수 있다고 함. 예를 들어 학습 데이터에서 본적 없는 '불어->한국어' 번역 문제를 어느정도 수준으로 해결!(물론 '불어->한국어' 번역 문제만을 학습한 번역 모델보다는 못하지만..)
=> 이것의 장점은 모델의 유연성을 부여한다는 것! 즉, GPT의 학습 데이터에 없는 문제라도 유연성을 발휘해 해결할 수 있을 가능성 존재! 이러한 것도 가능한 것은 방대한 양의 데이터를 학습했기 때문!

*하지만 무조건적으로 해결하는 것이 아닌 해결할 '가능성'이 있다는 것 뿐(마치 만병통치약 처럼 생각하지말자!)



